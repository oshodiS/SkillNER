{
  "setup.py": [],
  "docs\\source\\conf.py": [],
  "skillNer\\cleaner.py": [
    {
      "type": "FunctionDef",
      "name": "remove_punctuation",
      "md_content": [
        "**remove_punctuation**: The function of remove_punctuation is to remove punctuation from a given text.\n\n**parameters**:\n- text: str\n- list_punctuations: List[str], optional\n\n**Code Description**:\nThe remove_punctuation function takes a text input and an optional list of punctuations to remove from the text. It iterates through the list of punctuations and replaces each punctuation with a space in the text. Finally, it returns the cleaned text after removing all punctuations. Additionally, the function uses the strip() method to eliminate any extra spaces at the beginning or end of the text.\n\n**Note**:\nMake sure to import the function correctly from SkillNer.cleaner before using it in your code.\n\n**Output Example**:\nIf the input text is \"Hello there, I am SkillNer! Annoation, annotation, annotation ...\", the output after removing punctuations would be: \"Hello there  I am SkillNer  Annoation  annotation  annotation\"."
      ],
      "code_start_line": 23,
      "code_end_line": 53,
      "params": [
        "text",
        "list_punctuations"
      ],
      "have_return": true,
      "code_content": "def remove_punctuation(\n    text: str,\n    list_punctuations: List[str] = LIST_PUNCTUATIONS,\n) -> str:\n    \"\"\"To Remove punctuation from a given text.\n\n    Parameters\n    ----------\n    text : str\n        The text to clean.\n    list_punctuations: List[str], optional\n        A list that define the punctuations to remove, by default LIST_PUNCTUATIONS\n\n    Returns\n    -------\n    str\n        returns a the provided text after removing all punctuations\n\n    Examples\n    --------\n    >>> from SkillNer.cleaner import remove_punctuation\n    >>> text = \"Hello there, I am SkillNer! Annoation, annotation, annotation ...\"\n    >>> print(remove_punctuation(text))\n    Hello there  I am SkillNer  Annoation  annotation  annotation\n    \"\"\"\n\n    for punc in list_punctuations:\n        text = text.replace(punc, \" \")\n\n    # use .strip() to remove extra space in the begining/end of the text\n    return text.strip()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "remove_redundant",
      "md_content": [
        "**remove_redundant**: The function of remove_redundant is to remove phrases that appear frequently and cannot be used to infer skills.\n\n**parameters**:\n- text: str\n- list_redundant_words: List[str] (default value is S_GRAM_REDUNDANT)\n\n**Code Description**:\nThe `remove_redundant` function takes a text input and a list of redundant words or phrases to be removed from the text. It iterates through the list of redundant words and removes them from the text using the `replace` method. Finally, it returns the cleaned text after removing all redundant words provided in the `list_redundant_words`. Additionally, it uses the `strip()` method to eliminate any extra spaces at the beginning or end of the text.\n\n**Note**:\nEnsure that the `list_redundant_words` parameter contains phrases that need to be removed from the text. The function is case-sensitive.\n\n**Output Example**:\nInput: \"you have professional experience building React apps, you are familiar with version control using git and GitHub\"\nOutput: \"building React apps, familiar with version control using git and GitHub\""
      ],
      "code_start_line": 57,
      "code_end_line": 87,
      "params": [
        "text",
        "list_redundant_words"
      ],
      "have_return": true,
      "code_content": "def remove_redundant(\n    text: str,\n    list_redundant_words: List[str] = S_GRAM_REDUNDANT,\n) -> str:\n    \"\"\"To remove phrases that appear frequently and that can not be used to infere skills.\n\n    Parameters\n    ----------\n    text : str\n        The text to clean.\n    list_redundant_words : List[str], optional\n        The list of phrases to remove, by default S_GRAM_REDUNDANT\n\n    Returns\n    -------\n    str\n        returns text after removing all redundant words provided in `list_redundant_words`\n\n    Examples\n    --------\n    >>> from SkillNer.cleaner import remove_redundant\n    >>> text = \"you have professional experience building React apps, you are familiar with version control using git and GitHub\"\n    >>> print(remove_redundant(text))\n    building React apps,  familiar with version control using git and GitHub\n    \"\"\"\n\n    for phrase in list_redundant_words:\n        text = text.replace(phrase, \"\")\n\n    # use .strip() to remove extra space in the begining/end of the text\n    return text.strip()\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "stem_text",
      "md_content": [
        "**stem_text**: The function of stem_text is to stem a given text using a specified stemmer.\n\n**parameters**:\n- text: The text to be stemmed.\n- stemmer: The stemmer to be used when stemming text, default is PorterStemmer from NLTK.\n\n**Code Description**:\nThe stem_text function takes a text input and a stemmer (defaulting to PorterStemmer) and returns the stemmed text. It tokenizes the input text, stems each word using the specified stemmer, and then joins the stemmed words back into a single string. This function is useful for preprocessing text data by reducing words to their base or root form.\n\nIn the project, this function is utilized within the Text class constructor to stem each word in the transformed text obtained after cleaning. By stemming the words, it helps in standardizing the text data for further processing and analysis.\n\n**Note**: Ensure that the stemmer provided is compatible with NLTK's stemmer interface to avoid any errors in stemming the text.\n\n**Output Example**:\nInput: \"professional experience building React apps\"\nOutput: \"profession experi build react app\""
      ],
      "code_start_line": 91,
      "code_end_line": 117,
      "params": [
        "text",
        "stemmer"
      ],
      "have_return": true,
      "code_content": "def stem_text(\n    text: str,\n    stemmer=PorterStemmer(),\n) -> str:\n    \"\"\"To stem a text \n\n    Parameters\n    ----------\n    text : str\n        The text to be stemmed.\n    stemmer : stemmer loaded from nltk, optional\n        The stemmer to be used when stemming text, by default PorterStemmer()\n\n    Returns\n    -------\n    str\n        returns text after stemming it.\n\n    Examples\n    --------\n    >>> from SkillNer.cleaner import stem_text\n    >>> text = \"you have professional experience building React apps, you are familiar with version control using git and GitHub\"\n    >>> print(stem_text(text))\n    you have profession experi build react apps, you are familiar with version control use git and github\n    \"\"\"\n\n    return \" \".join([stemmer.stem(word) for word in text.split(\" \")])\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\text_class.py",
        "skillNer\\text_class.py/Text/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "lem_text",
      "md_content": [
        "**lem_text**: The function of lem_text is to lemmatize a given text input using a provided spaCy NLP object.\n\n**parameters**:\n- text: A string representing the text to be lemmatized.\n- nlp: An NLP object loaded from spaCy used to lemmatize the text.\n\n**Code Description**:\nThe lem_text function takes a text input and an NLP object as parameters. It processes the text using the provided NLP object to lemmatize the words in the text. Lemmatization is the process of reducing words to their base or root form. The function then returns the lemmatized text as a string.\n\n**Note**:\nMake sure to have spaCy library installed and loaded with the desired language model before using this function.\n\n**Output Example**:\nIf the input text is \"you have professional experience building React apps, you are familiar with version control using git and GitHub\", the function will return \"you have professional experience building react app , you be familiar with version control use git and GitHub\"."
      ],
      "code_start_line": 121,
      "code_end_line": 150,
      "params": [
        "text",
        "nlp"
      ],
      "have_return": true,
      "code_content": "def lem_text(\n    text: str,\n    nlp,\n) -> str:\n    \"\"\"To lem a text.\n\n    Parameters\n    ----------\n    text : str\n        the text to be lemmed\n    nlp : nlp object loaded form spacy.\n        the nlp used to lem the text\n\n    Returns\n    -------\n    str\n        returns text after lemming it.\n\n    Examples\n    --------\n    >>> from SkillNer.cleaner import lem_text\n    >>> import spacy\n    >>> nlp = spacy.load(\"en_core_web_sm\")\n    >>> text = \"you have professional experience building React apps, you are familiar with version control using git and GitHub\"\n    >>> print(lem_text(text, nlp))\n    you have professional experience building react app , you be familiar with version control use git and GitHub\n    \"\"\"\n\n    doc = nlp(text)\n    return ' '.join([token.lemma_ for token in doc])\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "remove_extra_space",
      "md_content": [
        "**remove_extra_space**: The function of remove_extra_space is to remove extra spaces in a given text.\n\n**parameters**: \n- text: str\n    The text to clean.\n\n**Code Description**: \nThe remove_extra_space function takes a string input text and removes any extra spaces within the text by splitting the text into words and then joining them back together with a single space between each word.\n\n**Note**: \nMake sure to pass a string as input to the function to remove extra spaces effectively.\n\n**Output Example**: \n\"I am sentence with a lot of annoying extra spaces.\""
      ],
      "code_start_line": 154,
      "code_end_line": 177,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "def remove_extra_space(\n    text: str,\n) -> str:\n    \"\"\"To remove extra space in a given text.\n\n    Parameters\n    ----------\n    text : str\n        The text to clean.\n\n    Returns\n    -------\n    str\n        returns text after removing all redundant spaces.\n\n    Examples\n    --------\n    >>> from SkillNer.cleaner import remove_extra_space\n    >>> text = \" I am   sentence with   a lot of  annoying extra    spaces    .\"\n    >>> print(remove_extra_space(text))\n    I am sentence with a lot of annoying extra spaces .\n    \"\"\"\n\n    return \" \".join(text.split())\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "find_index_phrase",
      "md_content": [
        "**find_index_phrase**: The function of find_index_phrase is to determine the indexes of words in a given phrase within a text.\n\n**Parameters**:\n- phrase: str\n  - the input phrase.\n- text: str\n  - the text where to look for the phrase and determine their indexes.\n\n**Code Description**:\nThe function takes a phrase and a text as input. It splits the text and the phrase into lists of words and compares them to find the indexes of the words in the phrase within the text. If the phrase is not found in the text, an empty list is returned.\n\nThe function is called within the Text class constructor in the text_class.py file to detect unmatchable words based on predefined redundant words. It plays a crucial role in setting the matchability of words within the text object.\n\n**Note**: \n- The function assumes that words in the phrase are separated by spaces.\n- The comparison is case-sensitive.\n\n**Output Example**:\nIf the phrase \"experience building\" is found in the text:\n```python\n[3, 4]\n```\nIf the phrase \"Hello World\" is not found in the text:\n```python\n[]\n```"
      ],
      "code_start_line": 192,
      "code_end_line": 233,
      "params": [
        "phrase",
        "text"
      ],
      "have_return": true,
      "code_content": "def find_index_phrase(\n    phrase: str,\n    text: str,\n) -> List[int]:\n    \"\"\"Function to determine the indexes of words in a phrase given a text.\n\n    Parameters\n    ----------\n    phrase : str\n        the input phrase.\n    text : str\n        the text where to look for phrase and detemine their indexes\n\n    Returns\n    -------\n    List[int]\n        returns a list of the indexes of words in phrase. An empty list is returned if phrase is not in text.\n\n    Examples\n    --------\n    >>> from SkillNer.cleaner import find_index_phrase\n    >>> text = \"you have professional experience building React apps, you are familiar with version control using git and GitHub\"\n    >>> phrase = \"experience building\"\n    >>> find_index_phrase(phrase, text)\n    [3, 4]\n    >>> find_index_phrase(phrase=\"Hello World\", text)\n    []\n    \"\"\"\n\n    if phrase in text:\n        # words in text\n        list_words = text.split(\" \")\n\n        # words in phrase\n        list_phrase_words = phrase.split(\" \")\n        n = len(list_phrase_words)\n\n        for i in range(len(text) - n):\n            if list_words[i:i + n] == list_phrase_words:\n                return [i + k for k in range(n)]\n\n    return []\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\text_class.py",
        "skillNer\\text_class.py/Text/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Cleaner",
      "md_content": [
        "**Cleaner**: The function of Cleaner is to build pipelines to clean text.\n\n**attributes**:\n- to_lowercase: whether to lowercase the text before cleaning it.\n- include_cleaning_functions: List of cleaning operations to include in the pipeline.\n- exclude_cleaning_function: List of cleaning operations to exclude for the pipeline.\n\n**Code Description**:\nThe Cleaner class is designed to create pipelines for text cleaning. It has an initializer method that allows customization of the cleaning process by specifying parameters such as whether to lowercase the text, the list of cleaning functions to include, and the list of functions to exclude.\n\nThe `__call__` method applies the initialized cleaning pipeline to a given text. It first converts the text to lowercase if specified. Then, it iterates through the cleaning functions based on the inclusion and exclusion lists to clean the text accordingly.\n\nIn the calling code from the Text class, an instance of Cleaner is used to preprocess the raw text by removing punctuation and extra spaces. The transformed text is then processed further to extract words and their metadata using an NLP object from Spacy.\n\n**Note**:\n- The Cleaner class provides flexibility in defining custom text cleaning pipelines.\n- Users can include or exclude specific cleaning functions based on their requirements.\n\n**Output Example**:\n'i am sentence with a lot of annoying extra spaces and some meaningless punctuation ah ah ah'"
      ],
      "code_start_line": 236,
      "code_end_line": 306,
      "params": [],
      "have_return": true,
      "code_content": "class Cleaner:\n    \"\"\"A class to build pipelines to clean text.\n    \"\"\"\n\n    def __init__(\n        self,\n        to_lowercase: bool = True,\n        include_cleaning_functions: List[str] = all_cleaning,\n        exclude_cleaning_function: List[str] = [],\n    ):\n        \"\"\"the constructor of the class.\n\n        Parameters\n        ----------\n        to_lowercase : bool, optional\n            whether to lowercase the text before cleaning it, by default True\n        include_cleaning_functions : List, optional\n            List of cleaning operations to include in the pipeline, by default all_cleaning\n        exclude_cleaning_function : List, optional\n            List of cleaning operations to exclude for the pipeline, by default []\n        \"\"\"\n\n        # store params\n        self.include_cleaning_functions = include_cleaning_functions\n        self.exclude_cleaning_functions = exclude_cleaning_function\n        self.to_lowercase = to_lowercase\n\n    def __call__(\n        self,\n        text: str\n    ) -> str:\n        \"\"\"To apply the initiallized cleaning pipeline on a given text.\n\n        Parameters\n        ----------\n        text : str\n            text to clean\n\n        Returns\n        -------\n        str\n            returns the text after applying all cleaning operations on it.\n\n        Examples\n        -------\n        >>> from skillNer.cleaner import Cleaner\n        >>> cleaner = Cleaner(\n                        to_lowercase=True,\n                        include_cleaning_functions=[\"remove_punctuation\", \"remove_extra_space\"]\n                    )\n        >>> text = \" I am   sentence with   a lot of  annoying extra    spaces    , and !! some ,., meaningless punctuation ?! .! AH AH AH\"\n        >>> cleaner(text)\n        'i am sentence with a lot of annoying extra spaces and some meaningless punctuation ah ah ah'\n        \"\"\"\n\n        # lower the provided text\n        if(self.to_lowercase):\n            text = text.lower()\n\n        # perform cleaning while ignoring exclude_cleaning_functions\n        if len(self.exclude_cleaning_functions):\n            for cleaning_name in dict_cleaning_functions.keys():\n                if cleaning_name not in self.exclude_cleaning_functions:\n                    text = dict_cleaning_functions[cleaning_name](text)\n        # if exclude_cleaning_functions was provided then include-cleaning_functions will be ignoned\n        else:\n            for cleaning_name in dict_cleaning_functions.keys():\n                if cleaning_name in self.include_cleaning_functions:\n                    text = dict_cleaning_functions[cleaning_name](text)\n\n        return text\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\text_class.py",
        "skillNer\\text_class.py/Text/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is the constructor of the class.\n\n**parameters**:\n- to_lowercase: bool, optional. Whether to lowercase the text before cleaning it, by default True.\n- include_cleaning_functions: List, optional. List of cleaning operations to include in the pipeline, by default all_cleaning.\n- exclude_cleaning_function: List, optional. List of cleaning operations to exclude for the pipeline, by default [].\n\n**Code Description**:\nThe __init__ function serves as the constructor for the class. It initializes the object with the provided parameters:\n- to_lowercase: A boolean value indicating whether the text should be converted to lowercase before cleaning.\n- include_cleaning_functions: A list of cleaning operations to be included in the cleaning pipeline. By default, it includes all cleaning operations.\n- exclude_cleaning_function: A list of cleaning operations to be excluded from the cleaning pipeline. By default, this list is empty.\n\nThe function stores these parameters as attributes of the class instance for later use.\n\n**Note**:\n- Ensure to provide the necessary parameters when initializing an instance of the class to customize the cleaning operations.\n- Modify the include_cleaning_functions and exclude_cleaning_function lists as needed to tailor the cleaning pipeline to specific requirements."
      ],
      "code_start_line": 240,
      "code_end_line": 261,
      "params": [
        "self",
        "to_lowercase",
        "include_cleaning_functions",
        "exclude_cleaning_function"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        to_lowercase: bool = True,\n        include_cleaning_functions: List[str] = all_cleaning,\n        exclude_cleaning_function: List[str] = [],\n    ):\n        \"\"\"the constructor of the class.\n\n        Parameters\n        ----------\n        to_lowercase : bool, optional\n            whether to lowercase the text before cleaning it, by default True\n        include_cleaning_functions : List, optional\n            List of cleaning operations to include in the pipeline, by default all_cleaning\n        exclude_cleaning_function : List, optional\n            List of cleaning operations to exclude for the pipeline, by default []\n        \"\"\"\n\n        # store params\n        self.include_cleaning_functions = include_cleaning_functions\n        self.exclude_cleaning_functions = exclude_cleaning_function\n        self.to_lowercase = to_lowercase\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__call__",
      "md_content": [
        "**__call__**: The function of __call__ is to apply the initialized cleaning pipeline on a given text.\n\n**parameters**:\n- text: str\n    - text to clean\n\n**Code Description**:\nThe __call__ function takes a text input and applies a series of cleaning operations based on the initialized parameters. It first converts the text to lowercase if the 'to_lowercase' parameter is set to True. Then, it iterates through the cleaning functions based on the 'exclude_cleaning_functions' and 'include_cleaning_functions' parameters. If 'exclude_cleaning_functions' is provided, the function applies all cleaning functions except those specified in the list. If 'include_cleaning_functions' is provided, only the specified cleaning functions are applied. The cleaned text is returned as output.\n\n**Note**:\n- Make sure to initialize the Cleaner object with the desired cleaning configurations before calling the __call__ function.\n- Ensure that the cleaning functions specified in 'exclude_cleaning_functions' and 'include_cleaning_functions' are valid functions available in the cleaning pipeline.\n\n**Output Example**:\n'i am sentence with a lot of annoying extra spaces and some meaningless punctuation ah ah ah'"
      ],
      "code_start_line": 263,
      "code_end_line": 306,
      "params": [
        "self",
        "text"
      ],
      "have_return": true,
      "code_content": "    def __call__(\n        self,\n        text: str\n    ) -> str:\n        \"\"\"To apply the initiallized cleaning pipeline on a given text.\n\n        Parameters\n        ----------\n        text : str\n            text to clean\n\n        Returns\n        -------\n        str\n            returns the text after applying all cleaning operations on it.\n\n        Examples\n        -------\n        >>> from skillNer.cleaner import Cleaner\n        >>> cleaner = Cleaner(\n                        to_lowercase=True,\n                        include_cleaning_functions=[\"remove_punctuation\", \"remove_extra_space\"]\n                    )\n        >>> text = \" I am   sentence with   a lot of  annoying extra    spaces    , and !! some ,., meaningless punctuation ?! .! AH AH AH\"\n        >>> cleaner(text)\n        'i am sentence with a lot of annoying extra spaces and some meaningless punctuation ah ah ah'\n        \"\"\"\n\n        # lower the provided text\n        if(self.to_lowercase):\n            text = text.lower()\n\n        # perform cleaning while ignoring exclude_cleaning_functions\n        if len(self.exclude_cleaning_functions):\n            for cleaning_name in dict_cleaning_functions.keys():\n                if cleaning_name not in self.exclude_cleaning_functions:\n                    text = dict_cleaning_functions[cleaning_name](text)\n        # if exclude_cleaning_functions was provided then include-cleaning_functions will be ignoned\n        else:\n            for cleaning_name in dict_cleaning_functions.keys():\n                if cleaning_name in self.include_cleaning_functions:\n                    text = dict_cleaning_functions[cleaning_name](text)\n\n        return text\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "skillNer\\general_params.py": [],
  "skillNer\\matcher_class.py": [
    {
      "type": "ClassDef",
      "name": "Matchers",
      "md_content": [
        "**Matchers**: The function of Matchers is to create and manage a pipeline of matchers used to annotate text with specific skills.\n\n**attributes**:\n- nlp: NLP object loaded from Spacy.\n- skills_db: A dictionary serving as a skill database for text annotation.\n- phraseMatcher: A PhraseMatcher object loaded using Spacy.\n\n**Code Description**:\nThe Matchers class is designed to instantiate a matcher pipeline for annotating text with skills. It contains methods to load different types of matchers and manage them within a pipeline. The constructor initializes the class with the necessary parameters such as the NLP object, skill database, and PhraseMatcher object. The class provides methods to load specific matchers based on inclusion or exclusion criteria, allowing for the creation of a customized pipeline. Additionally, it includes methods to create different types of matchers based on skill information stored in the skill database.\n\nThe load_matchers method loads matchers based on the specified inclusion and exclusion criteria, returning a dictionary of loaded matchers. The method allows for the creation of a pipeline by defining the order of matchers to be included.\n\nThe class includes methods to create different types of matchers:\n- get_full_matcher: Creates a matcher for full skill names.\n- get_abv_matcher: Creates a matcher for skill abbreviations.\n- get_full_uni_matcher: Creates a matcher for single-word skill names.\n- get_low_form_matcher: Creates a matcher for low-confidence skill forms.\n- get_token_matcher: Creates a matcher based on individual tokens of skills.\n\nThe Matchers class is essential for setting up a skill annotation system and provides flexibility in defining and customizing the annotation pipeline.\n\n**Note**: It is crucial to provide the necessary parameters (nlp, skills_db, phraseMatcher) when instantiating the Matchers class to ensure proper functionality.\n\n**Output Example**:\n{'full_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c9680b8ac0>,\n 'abv_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a6d0>,\n 'full_uni_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a7b0>,\n 'low_form_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a900>,\n 'token_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a820>}"
      ],
      "code_start_line": 9,
      "code_end_line": 217,
      "params": [],
      "have_return": true,
      "code_content": "class Matchers:\n    \"\"\"class to instanciate a matcher pipeline used to annotate text.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp,\n        skills_db: dict,\n        phraseMatcher\n    ):\n        \"\"\"Constructor of the class\n\n        Parameters\n        ----------\n        nlp : [type]\n            NLP object loaded from spacy\n        skills_db : dict\n            A skill database that serves as a lookup table to annotate text\n        phraseMatcher : [type]\n            a phraseMatcher loaded using spacy\n        \"\"\"\n\n        # params\n        self.nlp = nlp\n        self.skills_db = skills_db\n        self.phraseMatcher = phraseMatcher\n        #self.stop_words = stop_words\n\n        # save matchers in a dict\n        self.dict_matcher = {\n            'full_matcher': self.get_full_matcher,\n            'abv_matcher': self.get_abv_matcher,\n            'full_uni_matcher': self.get_full_uni_matcher,\n            'low_form_matcher': self.get_low_form_matcher,\n            'token_matcher': self.get_token_matcher,\n        }\n\n        return\n\n    # load specified matchers\n    def load_matchers(\n            self,\n            include: List[str] = ['full_matcher',\n                                  'abv_matcher',\n                                  'full_uni_matcher',\n                                  'low_form_matcher',\n                                  'token_matcher',\n                                  ],\n            exclude: List[str] = []) -> dict:\n        \"\"\"To load matchers. The order of matchers define a pipeline.\n\n        Parameters\n        ----------\n        include : List[str], optional\n            List of matchers to include in the pipeline, by default ['full_matcher', 'abv_matcher', 'full_uni_matcher', 'low_form_matcher', 'token_matcher', ]\n        exclude : List[str], optional\n            List of matchers to exclude from the pipeline, by default []\n\n        Returns\n        -------\n        dict\n            returns a dictionnary where the keys are the name of matchers and the values are the matchers\n\n        Examples\n        --------\n        >>> from skillNer.matcher_class import Matchers\n        >>> from skillNer.general_params import SKILL_DB\n        >>> import spacy\n        >>> from spacy.matcher import PhraseMatcher\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> matcher_pipeline = Matchers(nlp, SKILL_DB, PhraseMatcher)\n        >>> matcher_pipeline.load_matchers()\n        loading full_matcher ...\n        loading abv_matcher ...\n        loading full_uni_matcher ...\n        loading low_form_matcher ...\n        loading token_matcher ...\n\n        {'full_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c9680b8ac0>,\n         'abv_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a6d0>,\n         'full_uni_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a7b0>,\n         'low_form_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a900>,\n         'token_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a820>}\n        \"\"\"\n\n        # #where to store loaded matchers\n        loaded_matchers = {}\n\n        # load matchers in if exclude is not empty\n        # include will ignored\n        if len(exclude):\n            for matcher_name, matcher in self.dict_matcher.items():\n                if matcher_name not in exclude:\n                    print(f\"loading {matcher_name} ...\")\n                    loaded_matchers[matcher_name] = matcher()\n        else:\n            for matcher_name, matcher in self.dict_matcher.items():\n                if matcher_name in include:\n                    print(f\"loading {matcher_name} ...\")\n                    loaded_matchers[matcher_name] = matcher()\n\n        return loaded_matchers\n\n    # matchers\n    # high confident matchers\n    def get_full_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        full_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n\n            skill_len = skills_db[key]['skill_len']\n            if skill_len > 1:\n                skill_full_name = skills_db[key]['high_surfce_forms']['full']\n                # add to matcher\n                skill_full_name_spacy = nlp.make_doc(skill_full_name)\n                full_matcher.add(str(skill_id), [skill_full_name_spacy])\n\n        return full_matcher\n\n    def get_abv_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        abv_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n            # check if there is a skill abrv\n            if 'abv' in skills_db[key]['high_surfce_forms'].keys():\n                skill_abv = skills_db[key]['high_surfce_forms']['abv']\n                skill_abv_spacy = nlp.make_doc(skill_abv)\n                abv_matcher.add(str(skill_id), [skill_abv_spacy])\n\n        return abv_matcher\n\n    def get_full_uni_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        full_uni_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n\n            skill_len = skills_db[key]['skill_len']\n            if skill_len == 1:\n                skill_full_name = skills_db[key]['high_surfce_forms']['full']\n                # add to matcher\n                skill_full_name_spacy = nlp.make_doc(skill_full_name)\n                full_uni_matcher.add(str(skill_id), [skill_full_name_spacy])\n\n        return full_uni_matcher\n\n    # low confident matchers\n    def get_low_form_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        low_form_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n\n            # get skill info\n            skill_id = key\n            skill_len = skills_db[key]['skill_len']\n\n            low_surface_forms = skills_db[key]['low_surface_forms']\n            for form in low_surface_forms:\n                skill_form_spacy = nlp.make_doc(form)\n                low_form_matcher.add(str(skill_id), [skill_form_spacy])\n        return low_form_matcher\n\n    def get_token_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        token_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n            match_on_tokens = skills_db[key]['match_on_tokens']\n\n            if match_on_tokens:  # check if skill accept matches on its unique tokens\n                skill_lemmed = skills_db[key]['high_surfce_forms']['full']\n                skill_lemmed_tokens = skill_lemmed.split(' ')\n\n                # add tokens to matcher\n                for token in skill_lemmed_tokens:\n                    # give id that ref 1_gram matching\n                    if token.isdigit():\n                        pass\n                    else:\n                        id_ = skill_id\n                        token_matcher.add(str(id_), [nlp.make_doc(token)])\n\n        return token_matcher\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py",
        "skillNer\\skill_extractor_class.py/SkillExtractor/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the Matcher class with necessary parameters.\n\n**parameters**:\n- nlp: NLP object loaded from spacy.\n- skills_db: A dictionary representing a skill database used for text annotation.\n- phraseMatcher: A phraseMatcher object loaded using spacy.\n\n**Code Description**:\nThe __init__ function serves as the constructor for the Matcher class. It takes in the NLP object, a skill database, and a phraseMatcher object as parameters. These parameters are then assigned to the class attributes for further use. Additionally, the function creates a dictionary called 'dict_matcher' that stores various matcher functions such as 'get_full_matcher', 'get_abv_matcher', 'get_full_uni_matcher', 'get_low_form_matcher', and 'get_token_matcher'. These matcher functions are essential for different types of skill matching operations within the Matcher class.\n\nThe 'dict_matcher' dictionary allows easy access to different matcher functions based on their keys, providing flexibility and modularity to the Matcher class. By storing these functions in a dictionary, the class can efficiently utilize them based on specific requirements during text annotation and skill identification processes.\n\n**Note**:\nEnsure that the NLP object, skill database, and phraseMatcher are properly initialized before calling the __init__ function to avoid any errors. The 'dict_matcher' dictionary should be used to access specific matcher functions as needed within the Matcher class.\n\n**Output Example**:\nNo explicit return value as the function is used for initialization purposes."
      ],
      "code_start_line": 13,
      "code_end_line": 46,
      "params": [
        "self",
        "nlp",
        "skills_db",
        "phraseMatcher"
      ],
      "have_return": true,
      "code_content": "    def __init__(\n        self,\n        nlp,\n        skills_db: dict,\n        phraseMatcher\n    ):\n        \"\"\"Constructor of the class\n\n        Parameters\n        ----------\n        nlp : [type]\n            NLP object loaded from spacy\n        skills_db : dict\n            A skill database that serves as a lookup table to annotate text\n        phraseMatcher : [type]\n            a phraseMatcher loaded using spacy\n        \"\"\"\n\n        # params\n        self.nlp = nlp\n        self.skills_db = skills_db\n        self.phraseMatcher = phraseMatcher\n        #self.stop_words = stop_words\n\n        # save matchers in a dict\n        self.dict_matcher = {\n            'full_matcher': self.get_full_matcher,\n            'abv_matcher': self.get_abv_matcher,\n            'full_uni_matcher': self.get_full_uni_matcher,\n            'low_form_matcher': self.get_low_form_matcher,\n            'token_matcher': self.get_token_matcher,\n        }\n\n        return\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\matcher_class.py/Matchers/get_full_matcher",
        "skillNer\\matcher_class.py/Matchers/get_abv_matcher",
        "skillNer\\matcher_class.py/Matchers/get_full_uni_matcher",
        "skillNer\\matcher_class.py/Matchers/get_low_form_matcher",
        "skillNer\\matcher_class.py/Matchers/get_token_matcher"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "load_matchers",
      "md_content": [
        "**load_matchers**: The function of load_matchers is to load a set of matchers based on the specified criteria and return them as a dictionary.\n\n**parameters**:\n- include: List of matchers to include in the pipeline.\n- exclude: List of matchers to exclude from the pipeline.\n\n**Code Description**:\nThe `load_matchers` function loads matchers based on the provided criteria. If the `exclude` list is not empty, matchers not in the `exclude` list will be loaded. Otherwise, matchers in the `include` list will be loaded. The function returns a dictionary where the keys are the names of the matchers and the values are the matchers themselves.\n\nThis function is called within the `__init__` method of the `SkillExtractor` class in the `skill_extractor_class.py` file. In the `SkillExtractor` class, the `load_matchers` function is used to initialize matchers for skill extraction using the `Matchers` class.\n\n**Note**: Ensure that the `include` and `exclude` parameters are used correctly to control which matchers are loaded.\n\n**Output Example**:\n{\n    'full_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c9680b8ac0>,\n    'abv_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a6d0>,\n    'full_uni_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a7b0>,\n    'low_form_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a900>,\n    'token_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a820>\n}"
      ],
      "code_start_line": 49,
      "code_end_line": 110,
      "params": [
        "self",
        "include",
        "exclude"
      ],
      "have_return": true,
      "code_content": "    def load_matchers(\n            self,\n            include: List[str] = ['full_matcher',\n                                  'abv_matcher',\n                                  'full_uni_matcher',\n                                  'low_form_matcher',\n                                  'token_matcher',\n                                  ],\n            exclude: List[str] = []) -> dict:\n        \"\"\"To load matchers. The order of matchers define a pipeline.\n\n        Parameters\n        ----------\n        include : List[str], optional\n            List of matchers to include in the pipeline, by default ['full_matcher', 'abv_matcher', 'full_uni_matcher', 'low_form_matcher', 'token_matcher', ]\n        exclude : List[str], optional\n            List of matchers to exclude from the pipeline, by default []\n\n        Returns\n        -------\n        dict\n            returns a dictionnary where the keys are the name of matchers and the values are the matchers\n\n        Examples\n        --------\n        >>> from skillNer.matcher_class import Matchers\n        >>> from skillNer.general_params import SKILL_DB\n        >>> import spacy\n        >>> from spacy.matcher import PhraseMatcher\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> matcher_pipeline = Matchers(nlp, SKILL_DB, PhraseMatcher)\n        >>> matcher_pipeline.load_matchers()\n        loading full_matcher ...\n        loading abv_matcher ...\n        loading full_uni_matcher ...\n        loading low_form_matcher ...\n        loading token_matcher ...\n\n        {'full_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c9680b8ac0>,\n         'abv_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a6d0>,\n         'full_uni_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a7b0>,\n         'low_form_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a900>,\n         'token_matcher': <spacy.matcher.phrasematcher.PhraseMatcher at 0x1c96c90a820>}\n        \"\"\"\n\n        # #where to store loaded matchers\n        loaded_matchers = {}\n\n        # load matchers in if exclude is not empty\n        # include will ignored\n        if len(exclude):\n            for matcher_name, matcher in self.dict_matcher.items():\n                if matcher_name not in exclude:\n                    print(f\"loading {matcher_name} ...\")\n                    loaded_matchers[matcher_name] = matcher()\n        else:\n            for matcher_name, matcher in self.dict_matcher.items():\n                if matcher_name in include:\n                    print(f\"loading {matcher_name} ...\")\n                    loaded_matchers[matcher_name] = matcher()\n\n        return loaded_matchers\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_full_matcher",
      "md_content": [
        "**get_full_matcher**: The function of get_full_matcher is to create a full matcher for skills based on the provided skill database.\n\n**parameters**:\n- nlp: NLP object loaded from spacy.\n- skills_db: A dictionary representing a skill database used for text annotation.\n- phraseMatcher: A phraseMatcher object loaded using spacy.\n\n**Code Description**:\nThe get_full_matcher function initializes a full matcher using the provided NLP object, skill database, and phraseMatcher. It then iterates over the skills in the database, extracts relevant information such as skill ID and full name, and adds them to the matcher. Finally, it returns the populated full matcher.\n\nIn the calling object \"__init__\" in the Matcher class, the get_full_matcher function is stored in a dictionary of matchers along with other matcher functions like 'abv_matcher', 'full_uni_matcher', 'low_form_matcher', and 'token_matcher'. This allows easy access to the get_full_matcher function and other matchers within the class.\n\n**Note**:\nEnsure that the nlp, skills_db, and phraseMatcher objects are properly initialized before calling the get_full_matcher function to avoid any errors.\n\n**Output Example**:\nfull_matcher = get_full_matcher()\n# Output:\n# <spacy.matcher.phrasematcher.PhraseMatcher object at 0x7f9d1b1e4c40>"
      ],
      "code_start_line": 114,
      "code_end_line": 132,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_full_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        full_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n\n            skill_len = skills_db[key]['skill_len']\n            if skill_len > 1:\n                skill_full_name = skills_db[key]['high_surfce_forms']['full']\n                # add to matcher\n                skill_full_name_spacy = nlp.make_doc(skill_full_name)\n                full_matcher.add(str(skill_id), [skill_full_name_spacy])\n\n        return full_matcher\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py/Matchers/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_abv_matcher",
      "md_content": [
        "**get_abv_matcher**: The function of get_abv_matcher is to create a matcher specifically for abbreviations of skills based on the provided skill database.\n\n**parameters**:\n- self: The instance of the class.\n  \n**Code Description**:\nThe get_abv_matcher function initializes a matcher for skill abbreviations by iterating through the skills database. It extracts the abbreviation for each skill, converts it into a Spacy Doc object, and adds it to the abbreviation matcher. The function then returns the abbreviation matcher.\n\nThis function is called within the __init__ method of the Matchers class in the project. In the __init__ method, the get_abv_matcher function is stored in a dictionary of matchers along with other matcher functions for different purposes. This allows for easy access to the abbreviation matcher when needed.\n\n**Note**:\n- This function relies on the presence of a skills database with the appropriate structure to extract skill abbreviations.\n- Ensure that the Spacy library is correctly set up and loaded before calling this function.\n\n**Output Example**:\nabv_matcher = get_abv_matcher()"
      ],
      "code_start_line": 134,
      "code_end_line": 150,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_abv_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        abv_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n            # check if there is a skill abrv\n            if 'abv' in skills_db[key]['high_surfce_forms'].keys():\n                skill_abv = skills_db[key]['high_surfce_forms']['abv']\n                skill_abv_spacy = nlp.make_doc(skill_abv)\n                abv_matcher.add(str(skill_id), [skill_abv_spacy])\n\n        return abv_matcher\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py/Matchers/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_full_uni_matcher",
      "md_content": [
        "**get_full_uni_matcher**: The function of get_full_uni_matcher is to create a matcher for full skill names based on the provided skill database.\n\n**parameters**:\n- self: The instance of the class.\n  \n**Code Description**:\nThe get_full_uni_matcher function initializes a matcher for full skill names using the provided skill database and the spaCy nlp object. It iterates through the skills in the database, extracts the full skill name, and adds it to the matcher.\n\nThe function first retrieves the necessary parameters such as the spaCy nlp object, the skills database, and initializes a phrase matcher for lowercase matching. It then iterates through each skill in the skills database, extracts the skill ID and full skill name, and checks if the skill length is 1. If the length is 1, it creates a spaCy Doc object for the full skill name and adds it to the full_uni_matcher with the skill ID as the key.\n\nThe function returns the populated full_uni_matcher containing the mappings of skill IDs to their respective full skill names.\n\nThis function is a crucial part of the skill matching process in the system, as it prepares a matcher specifically for full skill names to be used in text annotation and identification.\n\n**Note**:\nDevelopers using this function should ensure that the skills database is properly formatted with the required information for skill matching to work correctly.\n\n**Output Example**:\nfull_uni_matcher = {\n    'skill_id1': [skill_full_name_spacy1],\n    'skill_id2': [skill_full_name_spacy2],\n    ...\n}"
      ],
      "code_start_line": 152,
      "code_end_line": 170,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_full_uni_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        full_uni_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n\n            skill_len = skills_db[key]['skill_len']\n            if skill_len == 1:\n                skill_full_name = skills_db[key]['high_surfce_forms']['full']\n                # add to matcher\n                skill_full_name_spacy = nlp.make_doc(skill_full_name)\n                full_uni_matcher.add(str(skill_id), [skill_full_name_spacy])\n\n        return full_uni_matcher\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py/Matchers/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_low_form_matcher",
      "md_content": [
        "**get_low_form_matcher**: The function of get_low_form_matcher is to create a matcher for low form surface skills based on the provided skill database.\n\n**parameters**:\n- self: The instance of the class.\n  \n**Code Description**:\nThe get_low_form_matcher function initializes a matcher for low form surface skills by iterating through the skills database. It retrieves the skill information such as skill ID, skill length, and low surface forms, then creates a Spacy document for each form and adds it to the low form matcher. Finally, it returns the populated low form matcher.\n\nThis function is called within the __init__ method of the Matchers class in the matcher_class.py file. In the __init__ method, the get_low_form_matcher function is included in a dictionary of matchers along with other matcher functions such as full_matcher, abv_matcher, full_uni_matcher, and token_matcher. This dictionary serves as a lookup table for different types of matchers used in the NLP application.\n\n**Note**:\nDevelopers can utilize the get_low_form_matcher function to create a specialized matcher for low form surface skills based on a provided skill database. It is essential to ensure that the skills database is correctly formatted to extract the required skill information.\n\n**Output Example**:\nlow_form_matcher = {\n    'skill_id1': [skill_form_spacy1],\n    'skill_id2': [skill_form_spacy2],\n    ...\n}"
      ],
      "code_start_line": 173,
      "code_end_line": 190,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_low_form_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        low_form_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n\n            # get skill info\n            skill_id = key\n            skill_len = skills_db[key]['skill_len']\n\n            low_surface_forms = skills_db[key]['low_surface_forms']\n            for form in low_surface_forms:\n                skill_form_spacy = nlp.make_doc(form)\n                low_form_matcher.add(str(skill_id), [skill_form_spacy])\n        return low_form_matcher\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py/Matchers/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_token_matcher",
      "md_content": [
        "**get_token_matcher**: The function of get_token_matcher is to create a token matcher based on the skills database provided.\n\n**parameters**:\n- self: The instance of the class.\n  \n**Code Description**:\nThe get_token_matcher function initializes a token matcher using the skills database. It iterates through the skills database, extracts relevant information such as skill ID and tokens to match on, and adds these tokens to the token matcher. Tokens are added to the matcher with an ID referencing the skill ID for 1-gram matching. The function then returns the populated token matcher.\n\nThis function is a crucial part of the Matcher class in the project. It is called within the __init__ method of the Matchers class to populate the token matcher used for matching skills in text.\n\n**Note**:\n- Ensure that the skills database provided contains the necessary information for matching tokens.\n- The token matcher created by this function is specific to the skills provided in the database.\n\n**Output Example**:\n```python\n{\n    'skill_id_1': [token1],\n    'skill_id_2': [token2, token3],\n    ...\n}\n```"
      ],
      "code_start_line": 192,
      "code_end_line": 217,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_token_matcher(self):\n        # params\n        nlp = self.nlp\n        skills_db = self.skills_db\n        token_matcher = self.phraseMatcher(nlp.vocab, attr=\"LOWER\")\n\n        # populate matcher\n        for key in skills_db:\n            # get skill info\n            skill_id = key\n            match_on_tokens = skills_db[key]['match_on_tokens']\n\n            if match_on_tokens:  # check if skill accept matches on its unique tokens\n                skill_lemmed = skills_db[key]['high_surfce_forms']['full']\n                skill_lemmed_tokens = skill_lemmed.split(' ')\n\n                # add tokens to matcher\n                for token in skill_lemmed_tokens:\n                    # give id that ref 1_gram matching\n                    if token.isdigit():\n                        pass\n                    else:\n                        id_ = skill_id\n                        token_matcher.add(str(id_), [nlp.make_doc(token)])\n\n        return token_matcher\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py/Matchers/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "SkillsGetter",
      "md_content": [
        "**SkillsGetter**: The function of SkillsGetter is to provide methods for extracting matched skills from text data.\n\n**attributes**:\n- nlp: Represents the NLP object used for text processing.\n\n**Code Description**:\nThe `SkillsGetter` class encapsulates functions for extracting matched skills from text data using different matching techniques. It contains the following methods:\n\n1. `get_full_match_skills(text_obj: Text, matcher)`: Extracts skills with full matches from the text.\n2. `get_abv_match_skills(text_obj: Text, matcher)`: Extracts skills with abbreviated matches from the text.\n3. `get_full_uni_match_skills(text_obj: Text, matcher)`: Extracts skills with full unigram matches from the text.\n4. `get_token_match_skills(text_obj: Text, matcher)`: Extracts skills based on token matches from the text.\n5. `get_low_match_skills(text_obj: Text, matcher)`: Extracts skills with low surface matches from the text.\n\nEach method takes a `Text` object and a `matcher` as input, processes the text data using the provided matcher, and returns a list of extracted skills along with the updated text object.\n\nThe `get_full_match_skills`, `get_abv_match_skills`, and `get_full_uni_match_skills` methods also update the `is_matchable` attribute of tokens in the text object to mark them as matched or unmatched.\n\nThe `SkillsGetter` class is designed to work in conjunction with other components such as the `Text` class for text representation and the matcher for skill matching.\n\nWhen called within the `SkillExtractor` class, an instance of `SkillsGetter` is initialized with the NLP object passed as a parameter.\n\n**Note**:\n- Ensure that the `Text` object passed to the methods contains the necessary text data for skill extraction.\n- The `matcher` parameter should be a valid matcher object compatible with the NLP library used.\n\n**Output Example**:\n```python\nskills = [{'skill_id': 'python_fullUni', 'score': 1, 'doc_node_value': 'Python', 'doc_node_id': [0], 'type': 'full_uni'}]\ntext_obj = <Updated Text Object>\n```"
      ],
      "code_start_line": 220,
      "code_end_line": 339,
      "params": [],
      "have_return": true,
      "code_content": "class SkillsGetter:\n    \"\"\"Class that gather functions to get the matched skills.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp\n    ):\n\n        # param\n        self.nlp = nlp\n        return\n\n    def get_full_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills = []\n        doc = self.nlp(text_obj.lemmed())\n\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            # add full_match to store\n            skills.append({'skill_id': id_,\n                           'doc_node_value': str(doc[start:end]),\n                           'score': 1,\n                           'doc_node_id': list(range(start, end))})\n            # mutate text tokens metadata (unmatch attr)\n            for token in text_obj[start:end]:\n                token.is_matchable = False\n\n        return skills, text_obj\n\n    def get_abv_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n        skills = []\n\n        doc = self.nlp(text_obj.abv_text)\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_,\n                               'score': 1,\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': [start]})\n                # mutate matched tokens\n                for token in text_obj[start:end]:\n                    token.is_matchable = False\n\n        return skills, text_obj\n\n    def get_full_uni_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills = []\n\n        doc = self.nlp(text_obj.transformed_text)\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_+'_fullUni',\n                               'score': 1,\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': [start],\n                               'type': 'full_uni'})\n\n        return skills, text_obj\n\n    def get_token_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills_full = []\n        skills = []\n        sub_matches = []\n        full_matches = []\n\n        doc = self.nlp(text_obj.lemmed())\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n\n            # add\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_+'_oneToken',\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': [start],\n                               'type': 'one_token'})\n\n        return skills\n\n    def get_low_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills = []\n        doc = self.nlp(text_obj.stemmed())\n\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            # handle skill in the end of phrase\n            start = start if start < len(text_obj) else start - 1\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_+'_lowSurf',\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': list(range(start, end)),\n                               'type': 'lw_surf'})\n\n        return skills, text_obj\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py",
        "skillNer\\skill_extractor_class.py/SkillExtractor/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the object with the provided 'nlp' parameter.\n\n**parameters**:\n- nlp: An object representing natural language processing functionalities.\n\n**Code Description**:\nThe __init__ function takes the 'nlp' parameter and assigns it to the object's 'nlp' attribute for further use.\n\n**Note**:\nEnsure that the 'nlp' parameter is a valid object with the necessary natural language processing capabilities to avoid errors during initialization.\n\n**Output Example**:\nNo output is generated explicitly from the __init__ function as it is used for initializing the object."
      ],
      "code_start_line": 224,
      "code_end_line": 231,
      "params": [
        "self",
        "nlp"
      ],
      "have_return": true,
      "code_content": "    def __init__(\n        self,\n        nlp\n    ):\n\n        # param\n        self.nlp = nlp\n        return\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_full_match_skills",
      "md_content": [
        "**get_full_match_skills**: The function of get_full_match_skills is to extract skills from a given text using a matcher object. It returns a list of dictionaries, where each dictionary represents a skill found in the text.\n\n**parameters**:\n- text_obj: Text - The text object that contains the raw text to be processed.\n- matcher: Matcher - The matcher object used to match skills in the text.\n\n**Code Description**:\nThe `get_full_match_skills` function takes a `text_obj` and a `matcher` as input. It initializes an empty list called `skills` to store the extracted skills. The function then processes the `text_obj` by calling its `lemmed` method, which returns the lemmed version of the text. The lemmed text is then passed to the `matcher` object, which matches skills in the text.\n\nThe function iterates over the matches found by the `matcher` object and extracts relevant information about each match. It retrieves the skill ID from the `matcher.vocab.strings` using the match ID. It also retrieves the matched text span from the `doc` object using the start and end indices of the match. The extracted information is then stored in a dictionary and appended to the `skills` list.\n\nAfter extracting the skills, the function mutates the `text_obj` by setting the `is_matchable` attribute of the matched tokens to False. This ensures that the matched tokens are not considered for further matching.\n\nFinally, the function returns a tuple containing the `skills` list and the mutated `text_obj`.\n\nThe `get_full_match_skills` function is called within the `annotate` method of the `SkillExtractor` class in the `skill_extractor_class.py` file. It is used to extract skills based on full matches in the input text.\n\n**Note**: It is important to provide the correct `text_obj` and `matcher` objects to the `get_full_match_skills` function for accurate skill extraction.\n\n**Output Example**:\n```python\ntext_obj = Text(\"Fluency in both English and French is mandatory\")\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"SKILL\", None, [{\"LOWER\": \"english\"}], [{\"LOWER\": \"french\"}])\nskills, mutated_text_obj = get_full_match_skills(text_obj, matcher)\nprint(skills)\n# Output: [{'skill_id': 'SKILL', 'doc_node_value': 'English', 'score': 1, 'doc_node_id': [3]}, {'skill_id': 'SKILL', 'doc_node_value': 'French', 'score': 1, 'doc_node_id': [5]}]\nprint(mutated_text_obj)\n# Output: fluency in both English and French is mandatory\n```"
      ],
      "code_start_line": 233,
      "code_end_line": 253,
      "params": [
        "self",
        "text_obj",
        "matcher"
      ],
      "have_return": true,
      "code_content": "    def get_full_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills = []\n        doc = self.nlp(text_obj.lemmed())\n\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            # add full_match to store\n            skills.append({'skill_id': id_,\n                           'doc_node_value': str(doc[start:end]),\n                           'score': 1,\n                           'doc_node_id': list(range(start, end))})\n            # mutate text tokens metadata (unmatch attr)\n            for token in text_obj[start:end]:\n                token.is_matchable = False\n\n        return skills, text_obj\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/annotate"
      ],
      "reference_who": [
        "skillNer\\text_class.py/Text",
        "skillNer\\text_class.py/Text/lemmed"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_abv_match_skills",
      "md_content": [
        "**get_abv_match_skills**: The function of get_abv_match_skills is to extract abbreviated skills from a given text object using a specified matcher.\n\n**Parameters**:\n- `text_obj`: A Text object that represents the input text.\n- `matcher`: A matcher object used to identify matches in the text.\n\n**Code Description**:\nThe `get_abv_match_skills` function takes a `text_obj` and a `matcher` as input parameters. It initializes an empty list called `skills` to store the extracted skills. \n\nThe function then applies the `nlp` method of the `text_obj` to preprocess the abbreviated text. It iterates over the matches found by the `matcher` in the preprocessed text. For each match, it retrieves the match ID and converts it to a string using the `vocab.strings` attribute of the `matcher`. \n\nIf the word at the start position of the match in the `text_obj` is matchable, it creates a dictionary containing the skill ID, a score of 1, the value of the matched tokens as a string, and the start position of the match. This dictionary is then appended to the `skills` list.\n\nNext, the function iterates over the tokens in the `text_obj` that were part of the match and sets their `is_matchable` attribute to False, indicating that they have been matched and should not be considered for further matches.\n\nFinally, the function returns the `skills` list and the modified `text_obj`.\n\n**Note**: The `get_abv_match_skills` function is called by the `annotate` method of the `SkillExtractor` class in the `skill_extractor_class.py` file.\n\n**Output Example**:\n```python\ntext_obj = Text(\"Fluency in both English and French is mandatory\")\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"SKILL\", None, [{\"LOWER\": \"english\"}])\nskills, modified_text_obj = get_abv_match_skills(text_obj, matcher)\nprint(skills)\n# Output: [{'skill_id': 'SKILL', 'score': 1, 'doc_node_value': 'English', 'doc_node_id': [3]}]\nprint(modified_text_obj)\n# Output: The modified text object with the matched tokens marked as unmatchable.\n```"
      ],
      "code_start_line": 255,
      "code_end_line": 274,
      "params": [
        "self",
        "text_obj",
        "matcher"
      ],
      "have_return": true,
      "code_content": "    def get_abv_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n        skills = []\n\n        doc = self.nlp(text_obj.abv_text)\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_,\n                               'score': 1,\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': [start]})\n                # mutate matched tokens\n                for token in text_obj[start:end]:\n                    token.is_matchable = False\n\n        return skills, text_obj\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/annotate"
      ],
      "reference_who": [
        "skillNer\\text_class.py/Text"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_full_uni_match_skills",
      "md_content": [
        "**get_full_uni_match_skills**: This function is responsible for extracting full unigram match skills from a given text.\n\n**Parameters**:\n- `text_obj`: An instance of the `Text` class representing the target text.\n- `matcher`: The matcher object used to perform the skill matching.\n\n**Code Description**:\nThe `get_full_uni_match_skills` function takes a `Text` object and a matcher object as input. It initializes an empty list called `skills` to store the extracted skills. Then, it processes the `text_obj` using the `nlp` object to obtain a processed document. \n\nNext, it iterates over the matches found by the `matcher` object in the processed document. For each match, it retrieves the match ID and converts it to a string using the `matcher.vocab.strings` attribute. It then checks if the word at the starting position of the match is matchable. If it is, it appends a dictionary to the `skills` list containing the skill ID, a score of 1, the value of the matched word, the starting position of the match, and the type of match.\n\nFinally, the function returns the `skills` list and the `text_obj`.\n\n**Note**: The `get_full_uni_match_skills` function is called by the `annotate` method of the `SkillExtractor` class in the `skill_extractor_class.py` file.\n\n**Output Example**:\n```python\nskills = [{'skill_id': 'KS123K75YYK8VGH90NCS_fullUni',\n           'score': 1,\n           'doc_node_value': 'english',\n           'doc_node_id': [3],\n           'type': 'full_uni'},\n          {'skill_id': 'KS1243976G466GV63ZBY_fullUni',\n           'score': 1,\n           'doc_node_value': 'french',\n           'doc_node_id': [5],\n           'type': 'full_uni'}]\ntext_obj = Text(\"Fluency in both English and French is mandatory\")\n```\n"
      ],
      "code_start_line": 276,
      "code_end_line": 294,
      "params": [
        "self",
        "text_obj",
        "matcher"
      ],
      "have_return": true,
      "code_content": "    def get_full_uni_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills = []\n\n        doc = self.nlp(text_obj.transformed_text)\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_+'_fullUni',\n                               'score': 1,\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': [start],\n                               'type': 'full_uni'})\n\n        return skills, text_obj\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/annotate"
      ],
      "reference_who": [
        "skillNer\\text_class.py/Text"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_token_match_skills",
      "md_content": [
        "**get_token_match_skills**: The function of get_token_match_skills is to extract skills from a given text based on token matches using a provided matcher.\n\n**parameters**:\n- text_obj: Text - The Text object that represents the input text.\n- matcher: Matcher - The matcher object used to find token matches in the text.\n\n**Code Description**:\nThe `get_token_match_skills` function takes a `Text` object and a `Matcher` object as input. It initializes empty lists for `skills_full`, `skills`, `sub_matches`, and `full_matches`. \n\nThe function then processes the `text_obj` by calling its `lemmed` method to get the lemmed version of the text. It uses the `nlp` object associated with the `Text` object to create a Spacy `doc` object.\n\nNext, the function iterates over the matches found by the `matcher` object in the `doc`. For each match, it retrieves the match ID and converts it to a string using the `matcher.vocab.strings` attribute. \n\nIf the word at the start position of the match in the `text_obj` is matchable (i.e., not a stop word), the function appends a dictionary to the `skills` list. This dictionary contains the skill ID, the value of the word in the `doc`, the start position of the word, and the type of the skill (which is set to 'one_token').\n\nFinally, the function returns the `skills` list, which contains the extracted skills based on token matches.\n\n**Note**: The `get_token_match_skills` function is called in the `annotate` method of the `SkillExtractor` class in the `skill_extractor_class.py` file. It is used to extract skills based on token matches from the input text.\n\n**Output Example**:\n```python\nimport spacy\nfrom skillNer.text_class import Text\n\nnlp = spacy.load('en_core_web_sm')\ntext_obj = Text(\"Fluency in both English and French is mandatory\")\nmatcher = nlp.matcher\nskills = text_obj.get_token_match_skills(text_obj, matcher)\nprint(skills)\n# Output: [{'skill_id': 'english_oneToken', 'doc_node_value': 'English', 'doc_node_id': [3], 'type': 'one_token'}, {'skill_id': 'french_oneToken', 'doc_node_value': 'French', 'doc_node_id': [5], 'type': 'one_token'}]\n```\n"
      ],
      "code_start_line": 296,
      "code_end_line": 318,
      "params": [
        "self",
        "text_obj",
        "matcher"
      ],
      "have_return": true,
      "code_content": "    def get_token_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills_full = []\n        skills = []\n        sub_matches = []\n        full_matches = []\n\n        doc = self.nlp(text_obj.lemmed())\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n\n            # add\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_+'_oneToken',\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': [start],\n                               'type': 'one_token'})\n\n        return skills\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/annotate"
      ],
      "reference_who": [
        "skillNer\\text_class.py/Text",
        "skillNer\\text_class.py/Text/lemmed"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_low_match_skills",
      "md_content": [
        "**get_low_match_skills**: The function of get_low_match_skills is to extract low match skills from a given text using a matcher.\n\n**parameters**:\n- text_obj: Text - An instance of the Text class representing the input text.\n- matcher: Matcher - A matcher object used to identify low match skills in the text.\n\n**Code Description**:\nThe `get_low_match_skills` function takes an instance of the `Text` class and a `matcher` object as input. It initializes an empty list called `skills` to store the extracted low match skills. The function then preprocesses the input text by calling the `stemmed` method of the `text_obj` instance, which returns the stemmed version of the text.\n\nNext, the function iterates over the matches found by the `matcher` object in the preprocessed text. For each match, it retrieves the match ID, start index, and end index. It then checks if the word at the start index is matchable, and if so, appends a dictionary containing information about the skill to the `skills` list. The dictionary includes the skill ID, the corresponding text node value, a list of the document node IDs, and the type of the skill.\n\nFinally, the function returns the `skills` list along with the original `text_obj`.\n\nThe `get_low_match_skills` function is called within the `annotate` method of the `SkillExtractor` class in the `skill_extractor_class.py` file. It is one of the steps involved in the skill extraction process, where low match skills are identified based on specific criteria. The extracted low match skills are then included in the final annotated results.\n\n**Note**:\n- Ensure that the `text_obj` parameter passed to the function is an instance of the `Text` class.\n- The `matcher` object should be instantiated and configured before calling the `get_low_match_skills` function.\n- The function relies on the `stemmed` method of the `text_obj` instance to preprocess the text before matching skills.\n- The extracted low match skills are appended to the `skills` list along with additional information such as the skill ID, text node value, document node IDs, and skill type.\n- The function returns the `skills` list and the original `text_obj`.\n\n**Output Example**:\n```python\nskills = [{'skill_id': 'KS123K75YYK8VGH90NCS_lowSurf',\n           'doc_node_value': 'english',\n           'doc_node_id': [3],\n           'type': 'lw_surf'},\n          {'skill_id': 'KS1243976G466GV63ZBY_lowSurf',\n           'doc_node_value': 'french',\n           'doc_node_id': [5],\n           'type': 'lw_surf'}]\ntext_obj = Text(\"Fluency in both English and French is mandatory\")\nresult = get_low_match_skills(text_obj, matcher)\nprint(result)\n# Output: ([{'skill_id': 'KS123K75YYK8VGH90NCS_lowSurf',\n#             'doc_node_value': 'english',\n#             'doc_node_id': [3],\n#             'type': 'lw_surf'},\n#            {'skill_id': 'KS1243976G466GV63ZBY_lowSurf',\n#             'doc_node_value': 'french',\n#             'doc_node_id': [5],\n#             'type': 'lw_surf'}], text_obj)\n```"
      ],
      "code_start_line": 320,
      "code_end_line": 339,
      "params": [
        "self",
        "text_obj",
        "matcher"
      ],
      "have_return": true,
      "code_content": "    def get_low_match_skills(\n        self,\n        text_obj: Text,\n        matcher\n    ):\n\n        skills = []\n        doc = self.nlp(text_obj.stemmed())\n\n        for match_id, start, end in matcher(doc):\n            id_ = matcher.vocab.strings[match_id]\n            # handle skill in the end of phrase\n            start = start if start < len(text_obj) else start - 1\n            if text_obj[start].is_matchable:\n                skills.append({'skill_id': id_+'_lowSurf',\n                               'doc_node_value': str(doc[start:end]),\n                               'doc_node_id': list(range(start, end)),\n                               'type': 'lw_surf'})\n\n        return skills, text_obj\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/annotate"
      ],
      "reference_who": [
        "skillNer\\text_class.py/Text",
        "skillNer\\text_class.py/Text/stemmed"
      ],
      "special_reference_type": [
        false,
        false
      ]
    }
  ],
  "skillNer\\skill_extractor_class.py": [
    {
      "type": "ClassDef",
      "name": "SkillExtractor",
      "md_content": [
        "**SkillExtractor**: The function of SkillExtractor is to annotate skills in a given text and visualize them.\n\n**attributes**:\n- nlp: NLP object loaded from spacy.\n- skills_db: A skill database used as a lookup table to annotate skills.\n- phraseMatcher: A phrasematcher loaded from spacy.\n- tranlsator_func: A function to translate text from the source language to English.\n\n**Code Description**:\nThe `SkillExtractor` class is designed to annotate skills in a given text and visualize them. It contains an `__init__` method to initialize the class attributes, including the NLP object, skill database, phrasematcher, and translation function. The `annotate` method processes the text, extracts skills, and returns a dictionary with the annotated skills. The `display` method renders the annotated skills using the `displacy` tool from spaCy. Lastly, the `describe` method provides more detailed information about the annotated skills using HTML, CSS, and JavaScript.\n\n**Note**:\n- Ensure that the necessary dependencies such as spaCy and the skill database are properly loaded before using the `SkillExtractor` class.\n- The `annotate` method can be used to extract skills from a given text and adjust the threshold for skill selection if needed.\n- The `display` method utilizes spaCy's `displacy` tool to visually represent the annotated skills.\n- The `describe` method offers a detailed view of the annotated skills using a combination of HTML, CSS, and JavaScript.\n\n**Output Example**:\n{\n    'text': 'fluency in both English and French is mandatory',\n    'results': {\n        'full_matches': [],\n        'ngram_scored': [\n            {\n                'skill_id': 'KS123K75YYK8VGH90NCS',\n                'doc_node_id': [3],\n                'doc_node_value': 'English',\n                'type': 'lowSurf',\n                'score': 1,\n                'len': 1\n            },\n            {\n                'skill_id': 'KS1243976G466GV63ZBY',\n                'doc_node_id': [5],\n                'doc_node_value': 'French',\n                'type': 'lowSurf',\n                'score': 1,\n                'len': 1\n            }\n        ]\n    }\n}"
      ],
      "code_start_line": 15,
      "code_end_line": 248,
      "params": [],
      "have_return": true,
      "code_content": "class SkillExtractor:\n    \"\"\"Main class to annotate skills in a given text and visualize them.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp,\n        skills_db,\n        phraseMatcher,\n        tranlsator_func=False\n    ):\n        \"\"\"Constructor of the class.\n\n        Parameters\n        ----------\n        nlp : [type]\n            NLP object loaded from spacy.\n        skills_db : [type]\n            A skill database used as a lookup table to annotate skills.\n        phraseMatcher : [type]\n            A phrasematcher loaded from spacy.\n        tranlsator_func :Callable\n            A fucntion to translate text from source language to english def tranlsator_func(text_input: str) -> text_input:str\n        \"\"\"\n\n        # params\n        self.tranlsator_func = tranlsator_func\n        self.nlp = nlp\n        self.skills_db = skills_db\n        self.phraseMatcher = phraseMatcher\n\n        # load matchers: all\n        self.matchers = Matchers(\n            self.nlp,\n            self.skills_db,\n            self.phraseMatcher,\n            # self.stop_words\n        ).load_matchers()\n\n        # init skill getters\n        self.skill_getters = SkillsGetter(self.nlp)\n\n        # init utils\n        self.utils = Utils(self.nlp, self.skills_db)\n        return\n\n    def annotate(\n        self,\n        text: str,\n        tresh: float = 0.5\n    ) -> dict:\n        \"\"\"To annotate a given text and thereby extract skills from it.\n\n        Parameters\n        ----------\n        text : str\n            The target text.\n        tresh : float, optional\n            A treshold used to select skills in case of confusion, by default 0.5\n\n        Returns\n        -------\n        dict\n            returns a dictionnary with the text that was used and the annotated skills (see example).\n\n        Examples\n        --------\n        >>> import spacy\n        >>> from spacy.matcher import PhraseMatcher\n        >>> from skillNer.skill_extractor_class import SkillExtractor\n        >>> from skillNer.general_params import SKILL_DB\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)\n        loading full_matcher ...\n        loading abv_matcher ...\n        loading full_uni_matcher ...\n        loading low_form_matcher ...\n        loading token_matcher ...\n        >>> text = \"Fluency in both english and french is mandatory\"\n        >>> skill_extractor.annotate(text)\n        {'text': 'fluency in both english and french is mandatory',\n        'results': {'full_matches': [],\n        'ngram_scored': [{'skill_id': 'KS123K75YYK8VGH90NCS',\n            'doc_node_id': [3],\n            'doc_node_value': 'english',\n            'type': 'lowSurf',\n            'score': 1,\n            'len': 1},\n        {'skill_id': 'KS1243976G466GV63ZBY',\n            'doc_node_id': [5],\n            'doc_node_value': 'french',\n            'type': 'lowSurf',\n            'score': 1,\n            'len': 1}]}}\n        \"\"\"\n\n        # check translator\n        if self.tranlsator_func:\n            text = self.tranlsator_func(text)\n\n        # create text object\n        text_obj = Text(text, self.nlp)\n        # get matches\n        skills_full, text_obj = self.skill_getters.get_full_match_skills(\n            text_obj, self.matchers['full_matcher'])\n\n        # tests\n\n        skills_abv, text_obj = self.skill_getters.get_abv_match_skills(\n            text_obj, self.matchers['abv_matcher'])\n\n        skills_uni_full, text_obj = self.skill_getters.get_full_uni_match_skills(\n            text_obj, self.matchers['full_uni_matcher'])\n\n        skills_low_form, text_obj = self.skill_getters.get_low_match_skills(\n            text_obj, self.matchers['low_form_matcher'])\n\n        skills_on_token = self.skill_getters.get_token_match_skills(\n            text_obj, self.matchers['token_matcher'])\n        full_sk = skills_full + skills_abv\n        # process pseudo submatchers output conflicts\n        to_process = skills_on_token + skills_low_form + skills_uni_full\n        process_n_gram = self.utils.process_n_gram(to_process, text_obj)\n\n        return {\n            'text': text_obj.transformed_text,\n            'results': {\n                'full_matches': full_sk,\n                'ngram_scored': [match for match in process_n_gram if match['score'] >= tresh],\n\n            }\n        }\n\n    def display(\n        self,\n        results: dict\n    ):\n        \"\"\"To display the annotated skills. \n        This method uses built-in classes of spacy to render annotated text, namely `displacy`.\n\n        Parameters\n        ----------\n        results : dict\n            results is the dictionnary resulting from applying `.annotate()` to a text.\n\n        Results\n        -------\n        None \n            render the text with annotated skills.\n        \"\"\"\n\n        # explode result object\n        text = results[\"text\"]\n        skill_extractor_results = results['results']\n\n        # words and their positions\n        words_position = Text.words_start_end_position(text)\n\n        # get matches\n        matches = []\n        for match_type in skill_extractor_results.keys():\n            for match in skill_extractor_results[match_type]:\n                matches.append(match)\n\n        # displacy render params\n        entities = []\n        colors = {}\n        colors_id = []\n\n        # fill params\n        for match in matches:\n            # skill id\n            skill_id = match[\"skill_id\"]\n\n            # index of words in skill\n            index_start_word, index_end_word = match['doc_node_id'][0], match['doc_node_id'][-1]\n\n            # build/append entity\n            entity = {\n                \"start\": words_position[index_start_word].start,\n                \"end\": words_position[index_end_word].end,\n                \"label\": self.skills_db[skill_id]['skill_name']\n            }\n            entities.append(entity)\n\n            # highlight matched skills\n            colors[entity['label']\n                   ] = SKILL_TO_COLOR[self.skills_db[skill_id]['skill_type']]\n            colors_id.append(entity['label'])\n\n        # prepare params\n        entities.sort(key=lambda x: x['start'], reverse=False)\n        options = {\"ents\": colors_id, \"colors\": colors}\n        ex = {\n            \"text\": text,\n            \"ents\": entities,\n            \"title\": None\n        }\n\n        # render\n        html = displacy.render(ex, style=\"ent\", manual=True, options=options)\n\n    def describe(\n        self,\n        annotations: dict\n    ):\n        \"\"\"To display more details about the annotated skills.\n        This method uses HTML, CSS, JavaScript combined with IPython to render the annotated skills.\n\n        Parameters\n        ----------\n        annotations : dict\n            annotations is the dictionnary resulting from applying `.annotate()` to a text.\n\n        Returns\n        -------\n        [type]\n            render text with annotated skills.\n        \"\"\"\n\n        # build phrases to display from annotations\n        arr_phrases = Phrase.split_text_to_phare(\n            annotations,\n            self.skills_db\n        )\n\n        # create DOM\n        document = DOM(children=[\n            render_phrase(phrase)\n            for phrase in arr_phrases\n        ])\n\n        # render\n        return document\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The constructor of the class.\n\n**Parameters**:\n- nlp: NLP object loaded from spacy.\n- skills_db: A skill database used as a lookup table to annotate skills.\n- phraseMatcher: A phrasematcher loaded from spacy.\n- tranlsator_func: A function to translate text from the source language to English. The function signature is `tranlsator_func(text_input: str) -> text_input:str`. (optional)\n\n**Code Description**:\nThe `__init__` function is the constructor of the `SkillExtractor` class. It initializes the class with the necessary parameters such as the NLP object, skill database, phraseMatcher, and an optional translator function. The constructor assigns the provided values to the corresponding attributes of the class.\n\nThe `nlp` parameter represents the NLP object loaded from the spacy library. This object is used for text processing and analysis.\n\nThe `skills_db` parameter is a skill database that serves as a lookup table to annotate skills. It contains information about different skills, such as their names, abbreviations, and surface forms.\n\nThe `phraseMatcher` parameter is a phrasematcher object loaded using the spacy library. It is used for matching skills in the text based on predefined patterns.\n\nThe `tranlsator_func` parameter is an optional callable function that can be used to translate text from the source language to English. The function should take a string as input and return the translated string. If this parameter is not provided, the translation functionality will be disabled.\n\nAfter assigning the parameters to the corresponding attributes, the constructor initializes other necessary objects and attributes for skill extraction. It loads matchers using the `Matchers` class, initializes skill getters using the `SkillsGetter` class, and initializes utility methods using the `Utils` class.\n\n**Note**: When creating an instance of the `SkillExtractor` class, make sure to provide the required parameters (nlp, skills_db, phraseMatcher) to ensure proper functionality. The translator function is optional and can be omitted if not needed.\n\n**Output Example**: None"
      ],
      "code_start_line": 19,
      "code_end_line": 59,
      "params": [
        "self",
        "nlp",
        "skills_db",
        "phraseMatcher",
        "tranlsator_func"
      ],
      "have_return": true,
      "code_content": "    def __init__(\n        self,\n        nlp,\n        skills_db,\n        phraseMatcher,\n        tranlsator_func=False\n    ):\n        \"\"\"Constructor of the class.\n\n        Parameters\n        ----------\n        nlp : [type]\n            NLP object loaded from spacy.\n        skills_db : [type]\n            A skill database used as a lookup table to annotate skills.\n        phraseMatcher : [type]\n            A phrasematcher loaded from spacy.\n        tranlsator_func :Callable\n            A fucntion to translate text from source language to english def tranlsator_func(text_input: str) -> text_input:str\n        \"\"\"\n\n        # params\n        self.tranlsator_func = tranlsator_func\n        self.nlp = nlp\n        self.skills_db = skills_db\n        self.phraseMatcher = phraseMatcher\n\n        # load matchers: all\n        self.matchers = Matchers(\n            self.nlp,\n            self.skills_db,\n            self.phraseMatcher,\n            # self.stop_words\n        ).load_matchers()\n\n        # init skill getters\n        self.skill_getters = SkillsGetter(self.nlp)\n\n        # init utils\n        self.utils = Utils(self.nlp, self.skills_db)\n        return\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\matcher_class.py/Matchers",
        "skillNer\\matcher_class.py/Matchers/load_matchers",
        "skillNer\\matcher_class.py/SkillsGetter",
        "skillNer\\utils.py/Utils"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "annotate",
      "md_content": [
        "**annotate**: The `annotate` function is used to annotate a given text and extract skills from it. It takes a target text as input and returns a dictionary with the annotated skills.\n\n**Parameters**:\n- `text` (str): The target text to be annotated.\n- `tresh` (float, optional): A threshold used to select skills in case of confusion. The default value is 0.5.\n\n**Code Description**:\nThe `annotate` function first checks if a translator function is provided. If so, it applies the translator function to the text.\n\nNext, it creates a `Text` object from the input text and the provided NLP object. This `Text` object represents the processed text.\n\nThe function then calls various methods from the `SkillGetter` object to extract skills based on different matchers. These methods include `get_full_match_skills`, `get_abv_match_skills`, `get_full_uni_match_skills`, `get_low_match_skills`, and `get_token_match_skills`. Each method extracts skills based on specific matching criteria and updates the `Text` object accordingly.\n\nAfter extracting the skills, the function processes the extracted skills using the `process_n_gram` method from the `Utils` object. This method resolves conflicts between skills and selects the best-matched skills based on specific conditions.\n\nFinally, the function returns a dictionary with the transformed text and the annotated skills. The annotated skills are categorized into \"full_matches\" and \"ngram_scored\".\n\n**Note**: It is important to provide the correct input text and threshold value for accurate skill extraction. The function relies on the `Text` object and various methods from the `SkillGetter` and `Utils` objects to perform the skill extraction and annotation.\n\n**Output Example**:\n```python\n{\n    'text': 'fluency in both english and french is mandatory',\n    'results': {\n        'full_matches': [],\n        'ngram_scored': [\n            {\n                'skill_id': 'KS123K75YYK8VGH90NCS',\n                'doc_node_id': [3],\n                'doc_node_value': 'english',\n                'type': 'lowSurf',\n                'score': 1,\n                'len': 1\n            },\n            {\n                'skill_id': 'KS1243976G466GV63ZBY',\n                'doc_node_id': [5],\n                'doc_node_value': 'french',\n                'type': 'lowSurf',\n                'score': 1,\n                'len': 1\n            }\n        ]\n    }\n}\n```\n\n**Note**: The above example is a mock-up of a possible output. The actual output may vary depending on the input text and the skill extraction process."
      ],
      "code_start_line": 61,
      "code_end_line": 146,
      "params": [
        "self",
        "text",
        "tresh"
      ],
      "have_return": true,
      "code_content": "    def annotate(\n        self,\n        text: str,\n        tresh: float = 0.5\n    ) -> dict:\n        \"\"\"To annotate a given text and thereby extract skills from it.\n\n        Parameters\n        ----------\n        text : str\n            The target text.\n        tresh : float, optional\n            A treshold used to select skills in case of confusion, by default 0.5\n\n        Returns\n        -------\n        dict\n            returns a dictionnary with the text that was used and the annotated skills (see example).\n\n        Examples\n        --------\n        >>> import spacy\n        >>> from spacy.matcher import PhraseMatcher\n        >>> from skillNer.skill_extractor_class import SkillExtractor\n        >>> from skillNer.general_params import SKILL_DB\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)\n        loading full_matcher ...\n        loading abv_matcher ...\n        loading full_uni_matcher ...\n        loading low_form_matcher ...\n        loading token_matcher ...\n        >>> text = \"Fluency in both english and french is mandatory\"\n        >>> skill_extractor.annotate(text)\n        {'text': 'fluency in both english and french is mandatory',\n        'results': {'full_matches': [],\n        'ngram_scored': [{'skill_id': 'KS123K75YYK8VGH90NCS',\n            'doc_node_id': [3],\n            'doc_node_value': 'english',\n            'type': 'lowSurf',\n            'score': 1,\n            'len': 1},\n        {'skill_id': 'KS1243976G466GV63ZBY',\n            'doc_node_id': [5],\n            'doc_node_value': 'french',\n            'type': 'lowSurf',\n            'score': 1,\n            'len': 1}]}}\n        \"\"\"\n\n        # check translator\n        if self.tranlsator_func:\n            text = self.tranlsator_func(text)\n\n        # create text object\n        text_obj = Text(text, self.nlp)\n        # get matches\n        skills_full, text_obj = self.skill_getters.get_full_match_skills(\n            text_obj, self.matchers['full_matcher'])\n\n        # tests\n\n        skills_abv, text_obj = self.skill_getters.get_abv_match_skills(\n            text_obj, self.matchers['abv_matcher'])\n\n        skills_uni_full, text_obj = self.skill_getters.get_full_uni_match_skills(\n            text_obj, self.matchers['full_uni_matcher'])\n\n        skills_low_form, text_obj = self.skill_getters.get_low_match_skills(\n            text_obj, self.matchers['low_form_matcher'])\n\n        skills_on_token = self.skill_getters.get_token_match_skills(\n            text_obj, self.matchers['token_matcher'])\n        full_sk = skills_full + skills_abv\n        # process pseudo submatchers output conflicts\n        to_process = skills_on_token + skills_low_form + skills_uni_full\n        process_n_gram = self.utils.process_n_gram(to_process, text_obj)\n\n        return {\n            'text': text_obj.transformed_text,\n            'results': {\n                'full_matches': full_sk,\n                'ngram_scored': [match for match in process_n_gram if match['score'] >= tresh],\n\n            }\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\matcher_class.py/SkillsGetter/get_full_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_abv_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_full_uni_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_token_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_low_match_skills",
        "skillNer\\text_class.py/Text",
        "skillNer\\utils.py/Utils/process_n_gram"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "display",
      "md_content": [
        "**display**: The function of display is to render the text with annotated skills using the built-in classes of spacy, specifically `displacy`.\n\n**parameters**:\n- results: dict - The dictionary resulting from applying `.annotate()` to a text.\n\n**Code Description**:\nThe `display` function is responsible for rendering the text with annotated skills. It takes the results of the skill extraction process as input, which is a dictionary containing the annotated text and the extracted skills. The function utilizes the built-in `displacy` class from the spacy library to visually represent the annotated skills in the text.\n\nFirst, the function extracts the necessary information from the `results` dictionary. It retrieves the original text and the extracted skill results. \n\nNext, it prepares the necessary parameters for the `displacy.render` method. It creates an empty list `entities` to store the entities (annotated skills) and dictionaries `colors` and `colors_id` to store the colors associated with each skill. \n\nThen, the function iterates through the extracted skill results and populates the `entities` list with the start and end positions of each skill in the text, along with the corresponding skill label. It also updates the `colors` dictionary with the skill label and its associated color based on the skill type.\n\nAfter preparing the necessary parameters, the function sorts the entities based on their start positions and creates an `options` dictionary with the entity labels and their corresponding colors.\n\nFinally, the function calls the `displacy.render` method with the prepared parameters to generate the HTML representation of the annotated text with highlighted skills.\n\n**Note**: \n- The `display` function relies on the `Text` class from the `text_class.py` file to extract the starting and ending positions of words in the text.\n- The `display` function requires the spacy library to be installed in order to use the `displacy` class for rendering the annotated text.\n- The `display` function does not return any value, it directly renders the annotated text.\n\n"
      ],
      "code_start_line": 148,
      "code_end_line": 215,
      "params": [
        "self",
        "results"
      ],
      "have_return": false,
      "code_content": "    def display(\n        self,\n        results: dict\n    ):\n        \"\"\"To display the annotated skills. \n        This method uses built-in classes of spacy to render annotated text, namely `displacy`.\n\n        Parameters\n        ----------\n        results : dict\n            results is the dictionnary resulting from applying `.annotate()` to a text.\n\n        Results\n        -------\n        None \n            render the text with annotated skills.\n        \"\"\"\n\n        # explode result object\n        text = results[\"text\"]\n        skill_extractor_results = results['results']\n\n        # words and their positions\n        words_position = Text.words_start_end_position(text)\n\n        # get matches\n        matches = []\n        for match_type in skill_extractor_results.keys():\n            for match in skill_extractor_results[match_type]:\n                matches.append(match)\n\n        # displacy render params\n        entities = []\n        colors = {}\n        colors_id = []\n\n        # fill params\n        for match in matches:\n            # skill id\n            skill_id = match[\"skill_id\"]\n\n            # index of words in skill\n            index_start_word, index_end_word = match['doc_node_id'][0], match['doc_node_id'][-1]\n\n            # build/append entity\n            entity = {\n                \"start\": words_position[index_start_word].start,\n                \"end\": words_position[index_end_word].end,\n                \"label\": self.skills_db[skill_id]['skill_name']\n            }\n            entities.append(entity)\n\n            # highlight matched skills\n            colors[entity['label']\n                   ] = SKILL_TO_COLOR[self.skills_db[skill_id]['skill_type']]\n            colors_id.append(entity['label'])\n\n        # prepare params\n        entities.sort(key=lambda x: x['start'], reverse=False)\n        options = {\"ents\": colors_id, \"colors\": colors}\n        ex = {\n            \"text\": text,\n            \"ents\": entities,\n            \"title\": None\n        }\n\n        # render\n        html = displacy.render(ex, style=\"ent\", manual=True, options=options)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\text_class.py/Text",
        "skillNer\\text_class.py/Text/words_start_end_position"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "describe",
      "md_content": [
        "**describe**: The function of `describe` is to display more details about the annotated skills by rendering them using HTML, CSS, JavaScript, and IPython.\n\n**Parameters**:\n- `annotations` (dict): The `annotations` parameter is a dictionary resulting from applying the `.annotate()` method to a text.\n\n**Code Description**:\nThe `describe` function takes in the `annotations` parameter, which is a dictionary containing information about the annotated skills in a text. It uses HTML, CSS, JavaScript, and IPython to render the annotated skills and display more details about them.\n\nThe function first calls the `split_text_to_phare` method of the `Phrase` class to split the text into skill and non-skill phrases based on the annotations and a skill database. This method returns a list of `Phrase` objects representing the skill and non-skill phrases.\n\nNext, the function creates a DOM (Document Object Model) using the `DOM` function from the `html_elements.py` module. The DOM represents the HTML document structure with specified children elements. The `children` parameter of the DOM function is a list comprehension that iterates over the `arr_phrases` list and calls the `render_phrase` function to generate the HTML representation of each `Phrase` object.\n\nThe `render_phrase` function, which is defined in the `html_elements.py` module, generates an HTML representation of a `Phrase` object. It checks if the phrase is a skill phrase by checking the `is_skill` attribute. If it is a skill phrase, it generates an HTML element with the skill phrase, skill type, and metadata components. It also includes JavaScript functions to handle mouse events for showing/hiding the metadata.\n\nFinally, the `describe` function returns the generated document, which is the result of rendering the annotated skills using HTML, CSS, JavaScript, and IPython.\n\n**Note**: When using the `describe` function, ensure that the `annotations` parameter is a valid dictionary resulting from applying the `.annotate()` method to a text.\n\n**Output Example**:\nThe output of the `describe` function is a rendered HTML document that displays the annotated skills with their details. Here is a mock-up example of the HTML structure:\n\n```html\n<head>\n    <link\n        id=\"external-css\"\n        rel=\"stylesheet\"\n        type=\"text/css\"\n        href=\"https://unpkg.com/tailwindcss@^2/dist/tailwind.min.css\"\n        media=\"all\"\n    />\n</head>\n<body>\n    <div id=\"root\" class=\"px-4 leading-10 mb-24\">\n        <span class='relative p-1 text-white rounded-md border bg-blue-500' onmouseleave='mouseLeaveHandler_123()' onmouseenter='mouseEnterHandler_123()'>\n            Skill Phrase\n            <span class='text-xs text-white font-bold'>(Certification)</span>\n            <div id='123' style='display: none;' class='absolute shadow-lg z-40 bg-white flex-col text-sm text-black p-2 border left-0 -bottom-15'>\n                <div class='flex grid-cols-2 gap-2 mb-4'>\n                    <span class='font-bold col-1'>Skill Name</span>\n                    <span class='col-1'>Python Programming</span>\n                </div>\n                <div class='flex grid-cols-2 gap-2 mb-4'>\n                    <span class='font-bold col-1'>Matching Type</span>\n                    <span class='col-1'>Exact Match</span>\n                </div>\n                <div class='flex grid-cols-2 gap-2 mb-4'>\n                    <span class='font-bold col-1'>Score</span>\n                    <span class='col-1'>0.85</span>\n                </div>\n            </div>\n            <script>\n                function mouseEnterHandler_123() {\n                    document.getElementById(\"123\").style.display = \"\";\n                }\n\n                function mouseLeaveHandler_123() {\n                    document.getElementById(\"123\").style.display = \"none\";\n                }\n            </script>\n        </span>\n        <span class='relative p-1 text-white rounded-md border bg-red-500' onmouseleave='mouseLeaveHandler_456()' onmouseenter='mouseEnterHandler_456()'>\n            Non-Skill Phrase\n        </span>\n    </div>\n</body>\n```\n\nPlease note that the above example is a simplified representation and the actual rendered HTML structure may vary based on the specific implementation and styling."
      ],
      "code_start_line": 217,
      "code_end_line": 248,
      "params": [
        "self",
        "annotations"
      ],
      "have_return": true,
      "code_content": "    def describe(\n        self,\n        annotations: dict\n    ):\n        \"\"\"To display more details about the annotated skills.\n        This method uses HTML, CSS, JavaScript combined with IPython to render the annotated skills.\n\n        Parameters\n        ----------\n        annotations : dict\n            annotations is the dictionnary resulting from applying `.annotate()` to a text.\n\n        Returns\n        -------\n        [type]\n            render text with annotated skills.\n        \"\"\"\n\n        # build phrases to display from annotations\n        arr_phrases = Phrase.split_text_to_phare(\n            annotations,\n            self.skills_db\n        )\n\n        # create DOM\n        document = DOM(children=[\n            render_phrase(phrase)\n            for phrase in arr_phrases\n        ])\n\n        # render\n        return document\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\visualizer\\html_elements.py/render_phrase",
        "skillNer\\visualizer\\html_elements.py/DOM",
        "skillNer\\visualizer\\phrase_class.py/Phrase",
        "skillNer\\visualizer\\phrase_class.py/Phrase/split_text_to_phare"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    }
  ],
  "skillNer\\text_class.py": [
    {
      "type": "ClassDef",
      "name": "Word",
      "md_content": [
        "**Word**: Main data structure to hold metadata of words.\n\n**attributes**:\n- word: str\n- lemmed: str\n- stemmed: str\n- is_stop_word: bool\n- is_matchable: bool\n- start: int\n- end: int\n\n**Code Description**:\nThe `Word` class serves as a data structure to store metadata related to words. Upon initialization, it takes a word as a string input and initializes various attributes such as lemmed, stemmed, is_stop_word, is_matchable, start, and end. The `metadata` method returns a dictionary containing all the metadata attributes of the word instance. The `__str__` method returns the raw form of the word, and the `__len__` method returns the number of characters in the word.\n\nIn the project, the `Word` class is utilized within the `Text` class. When a `Text` object is created, the text is processed to extract individual words using an NLP object. Each word is then represented as a `Word` object with its corresponding metadata. The `Text` class also provides a method to access a word at a specific position using the `__getitem__` method. Additionally, the `words_start_end_position` function in the `Text` class utilizes the `Word` class to determine the starting and ending index of each word in a given text.\n\n**Note**:\n- The `Word` class is designed to encapsulate word-related metadata and provide methods to access and manipulate this information.\n- Ensure that the necessary attributes are properly initialized when creating a `Word` object.\n\n**Output Example**:\n```python\nfrom skillNer.text_class import Word\n\nword_obj = Word(\"Hello\")\nprint(word_obj.metadata().keys())\n# dict_keys(['lemmed', 'stemmed', 'is_stop_word', 'is_matachable'])\n\nprint(word_obj)\n# Hello\n\nprint(len(word_obj))\n# 5\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 105,
      "params": [],
      "have_return": true,
      "code_content": "class Word:\n    \"\"\"Main data structure to hold metadata of words\n    \"\"\"\n\n    def __init__(\n        self,\n        word: str\n    ) -> None:\n        \"\"\"Construct an instance of Word\n\n        Parameters\n        ----------\n        word : str\n            The word is given as string\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        \"\"\"\n\n        # immutable version of word\n        self.word = word\n\n        # attributes\n        self.lemmed = \"\"\n        self.stemmed = \"\"\n\n        self.is_stop_word = None\n        self.is_matchable = True\n\n        # position in sentence\n        self.start: int\n        self.end: int\n        pass\n\n    # get metadata of word\n    def metadata(self) -> dict:\n        \"\"\"To get all metadata of the instance\n\n        Returns\n        -------\n        dict\n            dictionnary containing all metadata of object. Look at the example to see the returned keys\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        >>> word_obj.metadata().keys()\n        dict_keys(['lemmed', 'stemmed', 'is_stop_word', 'is_matachable'])\n        \"\"\"\n\n        return {\n            \"lemmed\": self.lemmed,\n            \"stemmed\": self.stemmed,\n            \"is_stop_word\": self.is_stop_word,\n            \"is_matachable\": self.is_matchable\n        }\n\n    # give the raw version of word when transformed to str\n    def __str__(self) -> str:\n        \"\"\"To get the raw form of word\n\n        Returns\n        -------\n        str\n            raw form of word\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        >>> print(word_obj)\n        Hello\n        \"\"\"\n        return self.word\n\n    # give the len of the word\n    def __len__(self) -> int:\n        \"\"\"Gives the number of characters in word\n\n        Returns\n        -------\n        int\n            returns the number of characters in word\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        >>> len(word_obj)\n        5\n        \"\"\"\n        return len(self.word)\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\text_class.py/Text/__init__",
        "skillNer\\text_class.py/Text/__getitem__",
        "skillNer\\text_class.py/Text/words_start_end_position"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the Word class with the provided word and set default attribute values.\n\n**parameters**:\n- word: str - The word provided as a string to initialize the Word instance.\n\n**Code Description**:\nThe __init__ function initializes an instance of the Word class with the given word. It assigns the provided word to the 'word' attribute. Additionally, it initializes the following attributes with default values:\n- lemmed: str - An empty string.\n- stemmed: str - An empty string.\n- is_stop_word: bool - None.\n- is_matchable: bool - True.\n- start: int - Uninitialized.\n- end: int - Uninitialized.\n\n**Note**:\n- Ensure to provide a string value for the 'word' parameter when creating an instance of the Word class.\n- The 'start' and 'end' attributes are left uninitialized and need to be set separately based on the position of the word in a sentence."
      ],
      "code_start_line": 15,
      "code_end_line": 45,
      "params": [
        "self",
        "word"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        word: str\n    ) -> None:\n        \"\"\"Construct an instance of Word\n\n        Parameters\n        ----------\n        word : str\n            The word is given as string\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        \"\"\"\n\n        # immutable version of word\n        self.word = word\n\n        # attributes\n        self.lemmed = \"\"\n        self.stemmed = \"\"\n\n        self.is_stop_word = None\n        self.is_matchable = True\n\n        # position in sentence\n        self.start: int\n        self.end: int\n        pass\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "metadata",
      "md_content": [
        "**metadata**: The function of metadata is to retrieve all metadata associated with the Word object.\n\n**parameters**: This function does not take any parameters.\n\n**Code Description**: The metadata function returns a dictionary containing the following keys and their corresponding values:\n- \"lemmed\": The lemmatized form of the word.\n- \"stemmed\": The stemmed form of the word.\n- \"is_stop_word\": A boolean indicating if the word is a stop word.\n- \"is_matchable\": A boolean indicating if the word is matchable.\n\n**Note**: Make sure to create an instance of the Word class before calling the metadata function to retrieve the metadata of a specific word.\n\n**Output Example**: \n{\n    \"lemmed\": \"hello\",\n    \"stemmed\": \"hello\",\n    \"is_stop_word\": False,\n    \"is_matchable\": True\n}"
      ],
      "code_start_line": 48,
      "code_end_line": 69,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def metadata(self) -> dict:\n        \"\"\"To get all metadata of the instance\n\n        Returns\n        -------\n        dict\n            dictionnary containing all metadata of object. Look at the example to see the returned keys\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        >>> word_obj.metadata().keys()\n        dict_keys(['lemmed', 'stemmed', 'is_stop_word', 'is_matachable'])\n        \"\"\"\n\n        return {\n            \"lemmed\": self.lemmed,\n            \"stemmed\": self.stemmed,\n            \"is_stop_word\": self.is_stop_word,\n            \"is_matachable\": self.is_matchable\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__str__",
      "md_content": [
        "**__str__**: The function of __str__ is to return the raw form of the word.\n\n**parameters**: \n- self: The instance of the Word class.\n\n**Code Description**: \nThe __str__ function in the Word class returns the raw form of the word stored in the object. When this function is called, it returns the word itself.\n\n**Note**: \nMake sure the Word class has a 'word' attribute that holds the raw form of the word.\n\n**Output Example**: \nIf the 'word' attribute of the Word object is \"Hello\", calling the __str__ function will return \"Hello\"."
      ],
      "code_start_line": 72,
      "code_end_line": 87,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __str__(self) -> str:\n        \"\"\"To get the raw form of word\n\n        Returns\n        -------\n        str\n            raw form of word\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        >>> print(word_obj)\n        Hello\n        \"\"\"\n        return self.word\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [
        "**__len__**: The function of __len__ is to return the number of characters in a word.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: The __len__ function calculates and returns the number of characters in the word stored in the object.\n\n**Note**: Make sure to instantiate the Word class with a word as a parameter before calling the __len__ function to get the correct count of characters.\n\n**Output Example**: \n5"
      ],
      "code_start_line": 90,
      "code_end_line": 105,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self) -> int:\n        \"\"\"Gives the number of characters in word\n\n        Returns\n        -------\n        int\n            returns the number of characters in word\n\n        Examples\n        --------\n        >>> from skillNer.text_class import Word\n        >>> word_obj = Word(\"Hello\")\n        >>> len(word_obj)\n        5\n        \"\"\"\n        return len(self.word)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "ClassDef",
      "name": "Text",
      "md_content": [
        "**Text**: The main object to store/preprocess a raw text.\n\n**Attributes**:\n- `immutable_text`: The immutable version of the raw text.\n- `transformed_text`: The transformed version of the text, which is lowercased, with punctuation removed and extra spaces removed.\n- `abv_text`: The transformed version of the text without lowercasing, used for abbreviation matching.\n- `list_words`: A list that holds all the words within the text.\n\n**Code Description**:\nThe `Text` class is the main object used to store and preprocess a raw text. It provides various methods to manipulate and extract information from the text.\n\nThe constructor `__init__` initializes the object with the raw text and an NLP object instantiated from Spacy. It performs the following tasks:\n- Stores the immutable version of the raw text in the `immutable_text` attribute.\n- Transforms the text by removing punctuation and extra spaces, and converts it to lowercase. The transformed text is stored in the `transformed_text` attribute.\n- Creates a version of the text without lowercasing, stored in the `abv_text` attribute.\n- Initializes an empty list `list_words` to hold the Word objects.\n\nThe `stemmed` method returns the stemmed version of the text. It takes an optional argument `as_list` which, if set to True, returns the stemmed words as a list. Otherwise, it returns the stemmed text as a string.\n\nThe `lemmed` method returns the lemmed version of the text. It also takes an optional argument `as_list` which, if set to True, returns the lemmed words as a list. Otherwise, it returns the lemmed text as a string.\n\nThe `__str__` method returns the raw version of the text as a string.\n\nThe `__getitem__` method allows accessing a word at a specific position by index. It returns the Word object at the specified index.\n\nThe `__len__` method returns the number of words in the text.\n\nThe `words_start_end_position` method takes a text as input and returns a list of Word objects, where each Word object contains the starting and ending index of a word in the text.\n\n**Note**: The `Text` class is used in the `get_full_match_skills`, `get_abv_match_skills`, `get_full_uni_match_skills`, `get_token_match_skills`, and `get_low_match_skills` methods of the `SkillsGetter` class in the `matcher_class.py` file. It is also used in the `annotate` method of the `SkillExtractor` class in the `skill_extractor_class.py` file. The `display` method of the `SkillExtractor` class also uses the `Text` class to process the annotated skills.\n\n**Output Example**:\n```python\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom skillNer.text_class import Text\ntext_obj = Text(\"Fluency in both English and French is mandatory\")\nprint(text_obj.stemmed())\n# Output: 'fluenci in both english and french is mandatori'\nprint(text_obj.stemmed(as_list=True))\n# Output: ['fluenci', 'in', 'both', 'english', 'and', 'french', 'is', 'mandatori']\nprint(text_obj.lemmed())\n# Output: 'fluency in both english and french be mandatory'\nprint(text_obj.lemmed(as_list=True))\n# Output: ['fluency', 'in', 'both', 'english', 'and', 'french', 'be', 'mandatory']\nprint(str(text_obj))\n# Output: 'Fluency in both English and French is mandatory'\nprint(text_obj[3])\n# Output: <skillNer.text_class.Word at 0x1cf13a9bd60>\nprint(len(text_obj))\n# Output: 8\nlist_words = Text.words_start_end_position(\"Hello World I am SkillNer\")\nword_1 = list_words[0]\nprint(word_1.start, word_1.end)\n# Output: 0 5\n```"
      ],
      "code_start_line": 108,
      "code_end_line": 371,
      "params": [],
      "have_return": true,
      "code_content": "class Text:\n    \"\"\"The main object to store/preprocess a raw text. \n    The object behaviour is like a list according to words.\n    \"\"\"\n\n    def __init__(\n        self,\n        text: str,\n        nlp\n    ):\n        \"\"\"Constructor of the class\n\n        Parameters\n        ----------\n        text : str\n            The raw text. It might be for instance a job description.\n        nlp : [type]\n            An NLP object instanciated from Spacy.\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        \"\"\"\n\n        # immutable version of text\n        self.immutable_text = text\n\n        # transformed text: lower + punctuation + extra space\n        # this is the version of text that we will be working with\n        cleaner = Cleaner(\n            include_cleaning_functions=[\n                \"remove_punctuation\",\n                \"remove_extra_space\"\n            ],\n            to_lowercase=False\n        )\n\n        self.transformed_text = cleaner(text).lower()\n        # abv version\n        self.abv_text = cleaner(text)\n\n        # list that holds all words within text\n        self.list_words = []\n\n        # construct list of words and create meta data object\n        doc = nlp(self.transformed_text)\n\n        for token in doc:\n            # create word object\n            word = Word(token.text)\n\n            # lem and stem\n            word.lemmed = token.lemma_\n            word.stemmed = stem_text(token.text)\n\n            # stop word and machability\n            word.is_stop_word = token.is_stop\n            # a stop word is unmatchable\n            if token.is_stop:\n                word.is_matchable = False\n\n            self.list_words.append(word)\n\n        # detect unmatchable words\n        for redundant_word in S_GRAM_REDUNDANT:\n            list_index = find_index_phrase(\n                phrase=redundant_word, text=self.transformed_text)\n\n            for index in list_index:\n                self[index].is_matchable = False\n\n    # return stemmed form of text either as str or list of words\n    def stemmed(\n        self,\n        as_list: bool = False\n    ):\n        \"\"\"To get the stemmed version of text\n\n        Parameters\n        ----------\n        as_list : bool (default False)\n            True to get a list of stemmed words within text. False, to get stemmed text in a form of string.\n\n        Returns\n        -------\n        str | List[str]\n            return the stemmed text in the specified form by the argument `as_list`.\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> text_obj.stemmed()\n        'fluenci in both english and french is mandatori'\n        >>> text_obj.stemmed(as_list=True)\n        ['fluenci', 'in', 'both', 'english', 'and', 'french', 'is', 'mandatori']\n        \"\"\"\n\n        list_stems = [word.stemmed for word in self.list_words]\n\n        if as_list:\n            return list_stems\n\n        return \" \".join(list_stems)\n\n    # return lemmed form of text either as str or list of words\n    def lemmed(\n        self,\n        as_list: bool = False\n    ):\n        \"\"\"To get the lemmed version of text\n\n        Parameters\n        ----------\n        as_list : bool\n            True to get a list of lemmed words within text. False, to get lemmed text in a form of string.\n\n        Returns\n        -------\n        str | List[str]\n            return the lemmed text in the specified form by the argument `as_list`\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> text_obj.lemmed()\n        'fluency in both english and french be mandatory'\n        >>> text_obj.lemmed(as_list=True)\n        ['fluency', 'in', 'both', 'english', 'and', 'french', 'be', 'mandatory']\n        \"\"\"\n\n        list_lems = [word.lemmed for word in self.list_words]\n\n        if as_list:\n            return list_lems\n\n        return \" \".join(list_lems)\n\n    # return raw version of text when converted to str\n    def __str__(self) -> str:\n        \"\"\"To get the raw version of text\n\n        Returns\n        -------\n        str\n            returns the raw version of text\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> print(text_obj)\n        Fluency in both English and French is mandatory\n        \"\"\"\n\n        return self.immutable_text\n\n    # equip text with the behavior of a list\n    # get item with []\n    def __getitem__(\n        self,\n        index: int\n    ) -> Word:\n        \"\"\"To get the word at the specified position by index\n\n        Parameters\n        ----------\n        index : int\n            the position of the word\n\n        Returns\n        -------\n        Word\n            returns thhe word object in the index-position\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> text_obj[3]\n        <skillNer.text_class.Word at 0x1cf13a9bd60>\n        >>> print(text_obj[3])\n        english\n        \"\"\"\n        return self.list_words[index]\n\n    # len of a text is the number of words in it\n    def __len__(self) -> int:\n        \"\"\"To get the number of words in text\n\n        Returns\n        -------\n        int\n            returns the number of words in text\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> len(text_obj)\n        8\n        \"\"\"\n\n        return len(self.list_words)\n\n    # result a list of word object\n    # each word contain the info of its start/end position\n    @staticmethod\n    def words_start_end_position(text: str) -> List[Word]:\n        \"\"\"To get the starting and ending index of each word in text\n\n        Parameters\n        ----------\n        text : str\n            The input text\n\n        Returns\n        -------\n        List[Word]\n            Returns a list of words where in each word the `start` and `end` \n            properties were filled by the starting and ending position of the word.\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> list_words = Text.words_start_end_position(\"Hello World I am SkillNer\")\n        >>> word_1 = list_words[0]\n        >>> print(word_1.start, word_1.end)\n        0 5\n        \"\"\"\n        # words in text\n        list_words = []\n\n        pointer = 0\n        for raw_word in text.split(\" \"):\n            # init object word\n            word = Word(raw_word)\n\n            # start and end of word\n            word.start = pointer\n            word.end = pointer + len(word)\n\n            # update pointer\n            pointer += len(word) + 1\n\n            list_words.append(word)\n\n        return list_words\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py",
        "skillNer\\matcher_class.py/SkillsGetter/get_full_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_abv_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_full_uni_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_token_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_low_match_skills",
        "skillNer\\skill_extractor_class.py",
        "skillNer\\skill_extractor_class.py/SkillExtractor/annotate",
        "skillNer\\skill_extractor_class.py/SkillExtractor/display",
        "skillNer\\utils.py",
        "skillNer\\utils.py/Utils/process_n_gram"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize an instance of the Text class.\n\n**Parameters**:\n- text: str\n  - The raw text. It could be a job description or any other text.\n- nlp: [type]\n  - An NLP object instantiated from Spacy.\n\n**Code Description**:\nThe __init__ function serves as the constructor of the Text class. It takes two parameters, \"text\" and \"nlp\". The \"text\" parameter represents the raw text that will be processed, while the \"nlp\" parameter is an NLP object instantiated from Spacy.\n\nUpon initialization, the function performs several operations to transform and extract information from the raw text. First, it assigns the input \"text\" to the \"immutable_text\" attribute, which represents an immutable version of the text.\n\nNext, a Cleaner object is created with specific cleaning functions to remove punctuation and extra spaces. The transformed text is then stored in the \"transformed_text\" attribute, which is the version of the text that will be used for further processing. Additionally, the \"abv_text\" attribute stores the transformed text without converting it to lowercase.\n\nThe function initializes an empty list called \"list_words\" to hold all the Word objects representing individual words within the text.\n\nTo extract words and their metadata, the function utilizes the NLP object by calling it with the \"transformed_text\" as input. It iterates through each token in the processed document and creates a Word object for each token. The Word object is initialized with the token's text and various metadata attributes such as \"lemmed\" (lemmatized form of the word), \"stemmed\" (stemmed form of the word), \"is_stop_word\" (whether the word is a stop word), and \"is_matchable\" (whether the word is matchable).\n\nAfter creating the Word objects, the function appends them to the \"list_words\" list.\n\nFinally, the function detects unmatchable words by checking if they match any predefined redundant words stored in the S_GRAM_REDUNDANT list. If a redundant word is found, the corresponding Word object's \"is_matchable\" attribute is set to False.\n\n**Note**:\n- The __init__ function is called when creating a new instance of the Text class.\n- The function relies on the Cleaner, Word, and stem_text objects from other modules in the project.\n- Ensure that the \"nlp\" object is properly instantiated from Spacy before calling the Text class constructor.\n- The function performs various text preprocessing steps, including cleaning, lemmatization, stemming, and identifying unmatchable words.\n- The resulting Text object can be used for further analysis and processing of the text data."
      ],
      "code_start_line": 113,
      "code_end_line": 180,
      "params": [
        "self",
        "text",
        "nlp"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        text: str,\n        nlp\n    ):\n        \"\"\"Constructor of the class\n\n        Parameters\n        ----------\n        text : str\n            The raw text. It might be for instance a job description.\n        nlp : [type]\n            An NLP object instanciated from Spacy.\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        \"\"\"\n\n        # immutable version of text\n        self.immutable_text = text\n\n        # transformed text: lower + punctuation + extra space\n        # this is the version of text that we will be working with\n        cleaner = Cleaner(\n            include_cleaning_functions=[\n                \"remove_punctuation\",\n                \"remove_extra_space\"\n            ],\n            to_lowercase=False\n        )\n\n        self.transformed_text = cleaner(text).lower()\n        # abv version\n        self.abv_text = cleaner(text)\n\n        # list that holds all words within text\n        self.list_words = []\n\n        # construct list of words and create meta data object\n        doc = nlp(self.transformed_text)\n\n        for token in doc:\n            # create word object\n            word = Word(token.text)\n\n            # lem and stem\n            word.lemmed = token.lemma_\n            word.stemmed = stem_text(token.text)\n\n            # stop word and machability\n            word.is_stop_word = token.is_stop\n            # a stop word is unmatchable\n            if token.is_stop:\n                word.is_matchable = False\n\n            self.list_words.append(word)\n\n        # detect unmatchable words\n        for redundant_word in S_GRAM_REDUNDANT:\n            list_index = find_index_phrase(\n                phrase=redundant_word, text=self.transformed_text)\n\n            for index in list_index:\n                self[index].is_matchable = False\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\cleaner.py/stem_text",
        "skillNer\\cleaner.py/find_index_phrase",
        "skillNer\\cleaner.py/Cleaner",
        "skillNer\\text_class.py/Word"
      ],
      "special_reference_type": [
        false,
        false,
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "stemmed",
      "md_content": [
        "**stemmed**: The function of stemmed is to return the stemmed version of text either as a string or a list of stemmed words based on the specified form.\n\n**parameters**:\n- as_list: bool (default False) - True to get a list of stemmed words within text. False, to get stemmed text in a form of string.\n\n**Code Description**:\nThe `stemmed` function takes the input text and returns the stemmed version of the text. It iterates through each word in the text, retrieves its stemmed form, and then based on the `as_list` parameter, either returns a string of stemmed text or a list of stemmed words. If `as_list` is set to True, it returns a list of stemmed words. Otherwise, it returns the stemmed text as a string. \n\nIn the calling object `get_low_match_skills` from `matcher_class.py`, the `stemmed` function is used to preprocess the text before matching skills. The stemmed text is obtained from the `text_obj` parameter, which is an instance of the `Text` class. The stemmed text is then processed using the `matcher` to identify low match skills based on specific criteria. The identified skills are appended to a list along with additional information and returned along with the original `text_obj`.\n\n**Note**: \n- Ensure that the `text_obj` passed to the function is an instance of the `Text` class.\n- The stemmed text is used for matching skills based on specific criteria.\n\n**Output Example**:\n- Example 1:\n    Input: text_obj.stemmed()\n    Output: 'fluenci in both english and french is mandatori'\n\n- Example 2:\n    Input: text_obj.stemmed(as_list=True)\n    Output: ['fluenci', 'in', 'both', 'english', 'and', 'french', 'is', 'mandatori']"
      ],
      "code_start_line": 183,
      "code_end_line": 216,
      "params": [
        "self",
        "as_list"
      ],
      "have_return": true,
      "code_content": "    def stemmed(\n        self,\n        as_list: bool = False\n    ):\n        \"\"\"To get the stemmed version of text\n\n        Parameters\n        ----------\n        as_list : bool (default False)\n            True to get a list of stemmed words within text. False, to get stemmed text in a form of string.\n\n        Returns\n        -------\n        str | List[str]\n            return the stemmed text in the specified form by the argument `as_list`.\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> text_obj.stemmed()\n        'fluenci in both english and french is mandatori'\n        >>> text_obj.stemmed(as_list=True)\n        ['fluenci', 'in', 'both', 'english', 'and', 'french', 'is', 'mandatori']\n        \"\"\"\n\n        list_stems = [word.stemmed for word in self.list_words]\n\n        if as_list:\n            return list_stems\n\n        return \" \".join(list_stems)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py/SkillsGetter/get_low_match_skills"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "lemmed",
      "md_content": [
        "**lemmed**: The function of lemmed is to return the lemmed version of text either as a string or a list of lemmed words based on the specified argument.\n\n**parameters**:\n- as_list: bool (default = False) - True to get a list of lemmed words within the text, False to get the lemmed text as a string.\n\n**Code Description**:\nThe `lemmed` function processes the text to return the lemmed version of the input text. It iterates over each word in the text and retrieves the lemmed form of the word. The function then either returns the lemmed text as a string or as a list of lemmed words based on the value of the `as_list` parameter.\n\nIn the provided code, the function is utilized in the `get_full_match_skills` and `get_token_match_skills` functions within the `SkillsGetter` class of the `matcher_class.py` file. These functions use the lemmed text to extract skills based on full matches and token matches, respectively. The `process_n_gram` function in the `utils.py` file also utilizes the `lemmed` function to process conflicted matches and choose the skills to keep based on the lemmed text.\n\n**Note**: Ensure to provide the correct text object to the `lemmed` function for accurate lemmed text extraction.\n\n**Output Example**:\n- Example 1:\n    ```python\n    import spacy\n    nlp = spacy.load('en_core_web_sm')\n    from skillNer.text_class import Text\n    text_obj = Text(\"Fluency in both English and French is mandatory\")\n    text_obj.lemmed()\n    # Output: 'fluency in both english and french be mandatory'\n    ```\n\n- Example 2:\n    ```python\n    import spacy\n    nlp = spacy.load('en_core_web_sm')\n    from skillNer.text_class import Text\n    text_obj = Text(\"Fluency in both English and French is mandatory\")\n    text_obj.lemmed(as_list=True)\n    # Output: ['fluency', 'in', 'both', 'english', 'and', 'french', 'be', 'mandatory']\n    ```"
      ],
      "code_start_line": 219,
      "code_end_line": 252,
      "params": [
        "self",
        "as_list"
      ],
      "have_return": true,
      "code_content": "    def lemmed(\n        self,\n        as_list: bool = False\n    ):\n        \"\"\"To get the lemmed version of text\n\n        Parameters\n        ----------\n        as_list : bool\n            True to get a list of lemmed words within text. False, to get lemmed text in a form of string.\n\n        Returns\n        -------\n        str | List[str]\n            return the lemmed text in the specified form by the argument `as_list`\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> text_obj.lemmed()\n        'fluency in both english and french be mandatory'\n        >>> text_obj.lemmed(as_list=True)\n        ['fluency', 'in', 'both', 'english', 'and', 'french', 'be', 'mandatory']\n        \"\"\"\n\n        list_lems = [word.lemmed for word in self.list_words]\n\n        if as_list:\n            return list_lems\n\n        return \" \".join(list_lems)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\matcher_class.py/SkillsGetter/get_full_match_skills",
        "skillNer\\matcher_class.py/SkillsGetter/get_token_match_skills",
        "skillNer\\utils.py/Utils/process_n_gram"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__str__",
      "md_content": [
        "**__str__**: The function of __str__ is to return the raw version of text.\n\n**parameters**: This Function does not take any parameters.\n\n**Code Description**: The __str__ function in the Text class returns the raw version of text stored in the object. It simply returns the value of the attribute \"immutable_text\".\n\n**Note**: Make sure the \"immutable_text\" attribute is properly set before calling this function to ensure the correct output.\n\n**Output Example**: \nIf the \"immutable_text\" attribute of the Text object is set to \"Fluency in both English and French is mandatory\", calling the __str__ function will return:\n\"Fluency in both English and French is mandatory\""
      ],
      "code_start_line": 255,
      "code_end_line": 273,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __str__(self) -> str:\n        \"\"\"To get the raw version of text\n\n        Returns\n        -------\n        str\n            returns the raw version of text\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> print(text_obj)\n        Fluency in both English and French is mandatory\n        \"\"\"\n\n        return self.immutable_text\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [
        "**__getitem__**: The function of __getitem__ is to retrieve the word at the specified position by index.\n\n**Parameters**:\n- index: int\n    the position of the word\n\n**Code Description**:\nThe `__getitem__` function in the `Text` class allows users to access a word at a specific position within the text. When called with an index parameter, it returns the `Word` object representing the word at that position. The function retrieves the word from the list of words stored in the `Text` object based on the provided index.\n\nThe `__getitem__` function is utilized in scenarios where direct access to individual words within a text object is required. By passing the index of the desired word, users can retrieve the corresponding `Word` object containing metadata such as the word itself, lemmatized and stemmed forms, stop word status, and positional information.\n\nIn the provided code example, an instance of the `Text` class is created with a sample text, and the `__getitem__` function is used to access the word at index 3. The function returns the `Word` object representing the word at that position, allowing further operations or analysis on the specific word.\n\n**Note**:\n- Ensure that the index provided is within the valid range of words in the text to avoid index out of range errors.\n- The `__getitem__` function provides a convenient way to retrieve individual words for processing or analysis within a text object.\n\n**Output Example**:\n```python\nimport spacy\nfrom skillNer.text_class import Text\n\ntext_obj = Text(\"Fluency in both English and French is mandatory\")\nprint(text_obj[3])\n# <skillNer.text_class.Word at 0x1cf13a9bd60>\n\nprint(text_obj[3].word)\n# English\n```"
      ],
      "code_start_line": 277,
      "code_end_line": 304,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(\n        self,\n        index: int\n    ) -> Word:\n        \"\"\"To get the word at the specified position by index\n\n        Parameters\n        ----------\n        index : int\n            the position of the word\n\n        Returns\n        -------\n        Word\n            returns thhe word object in the index-position\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> text_obj[3]\n        <skillNer.text_class.Word at 0x1cf13a9bd60>\n        >>> print(text_obj[3])\n        english\n        \"\"\"\n        return self.list_words[index]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\text_class.py/Word"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [
        "**__len__**: The function of __len__ is to return the number of words in the text.\n\n**parameters**: \n- No external parameters are required for this function.\n\n**Code Description**: \nThe __len__ function calculates and returns the number of words in the text by accessing the list of words stored in the object.\n\n**Note**: \nEnsure that the object has been properly initialized with the text content before calling this function to accurately count the words.\n\n**Output Example**: \n8"
      ],
      "code_start_line": 307,
      "code_end_line": 325,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self) -> int:\n        \"\"\"To get the number of words in text\n\n        Returns\n        -------\n        int\n            returns the number of words in text\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> text_obj = Text(\"Fluency in both English and French is mandatory\")\n        >>> len(text_obj)\n        8\n        \"\"\"\n\n        return len(self.list_words)\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "words_start_end_position",
      "md_content": [
        "**words_start_end_position**: The function of words_start_end_position is to extract the starting and ending index of each word in a given text.\n\n**parameters**:\n- text: str - The input text for which the starting and ending positions of words need to be determined.\n\n**Code Description**:\nThe `words_start_end_position` function processes the input text to identify individual words and their corresponding starting and ending positions. It iterates through the words in the text, calculates the start and end positions of each word, and stores this information in a list of `Word` objects. Each `Word` object encapsulates the word itself along with its start and end positions. The function then returns a list containing these `Word` objects with the calculated positions.\n\nThis function relies on the `Word` class from the `Text` module to create and manage word objects with specific metadata. By utilizing the `Word` class, the `words_start_end_position` function ensures accurate tracking of word positions within the text.\n\nWhen called, this function can be used to extract detailed positional information about individual words in a text, facilitating further text processing and analysis tasks.\n\n**Note**:\n- Ensure that the input text is a valid string for accurate word position extraction.\n- The function returns a list of `Word` objects, each containing the word, start position, and end position.\n\n**Output Example**:\n```python\nfrom skillNer.text_class import Text\n\nlist_words = Text.words_start_end_position(\"Hello World I am SkillNer\")\nword_1 = list_words[0]\nprint(word_1.start, word_1.end)\n# Output: 0 5\n```"
      ],
      "code_start_line": 330,
      "code_end_line": 371,
      "params": [
        "text"
      ],
      "have_return": true,
      "code_content": "    def words_start_end_position(text: str) -> List[Word]:\n        \"\"\"To get the starting and ending index of each word in text\n\n        Parameters\n        ----------\n        text : str\n            The input text\n\n        Returns\n        -------\n        List[Word]\n            Returns a list of words where in each word the `start` and `end` \n            properties were filled by the starting and ending position of the word.\n\n        Examples\n        --------\n        >>> import spacy\n        >>> nlp = spacy.load('en_core_web_sm')\n        >>> from skillNer.text_class import Text\n        >>> list_words = Text.words_start_end_position(\"Hello World I am SkillNer\")\n        >>> word_1 = list_words[0]\n        >>> print(word_1.start, word_1.end)\n        0 5\n        \"\"\"\n        # words in text\n        list_words = []\n\n        pointer = 0\n        for raw_word in text.split(\" \"):\n            # init object word\n            word = Word(raw_word)\n\n            # start and end of word\n            word.start = pointer\n            word.end = pointer + len(word)\n\n            # update pointer\n            pointer += len(word) + 1\n\n            list_words.append(word)\n\n        return list_words\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/display"
      ],
      "reference_who": [
        "skillNer\\text_class.py/Word"
      ],
      "special_reference_type": [
        true
      ]
    }
  ],
  "skillNer\\utils.py": [
    {
      "type": "ClassDef",
      "name": "Utils",
      "md_content": [
        "**Utils**: The function of Utils is to provide utility methods for processing skills and text data.\n\n**attributes**:\n- nlp: The NLP object used for text processing.\n- skills_db: A database containing information about skills.\n- token_dist: A constant representing token distance.\n- sign: A function for mathematical operations.\n  \n**Code Description**:\nThe `Utils` class contains methods for various utility operations. The `make_one` method creates a binary list based on a given cluster and length. The `split_at_values` method splits a list at specified values. The `grouper` method groups items based on a given distance. The `get_clusters` method extracts clusters from a co-occurrence matrix. The `get_corpus` method generates a binary matrix for future computations. The `one_gram_sim` method calculates similarity between text and skill strings. The `compute_w_ratio` method computes a ratio based on matched tokens. The `retain` method filters and scores text spans based on skills. The `process_n_gram` method processes conflicted matches to choose the ones to keep.\n\nThe `Utils` class is instantiated with an NLP object and a skills database. It provides essential functions for handling skills and text data, such as creating binary matrices, calculating similarity, and processing text spans. The class interacts with the `SkillExtractor` class in the project to support skill extraction and matching functionalities.\n\n**Note**:\nDevelopers can utilize the `Utils` class to perform various utility operations related to skills and text processing in the project.\n\n**Output Example**:\n```python\n[\n    {\n        'skill_id': 123,\n        'doc_node_id': [1, 3, 5],\n        'doc_node_value': 'example text',\n        'type': 'oneToken',\n        'score': 0.8,\n        'len': 3\n    },\n    ...\n]\n```"
      ],
      "code_start_line": 17,
      "code_end_line": 241,
      "params": [],
      "have_return": true,
      "code_content": "class Utils:\n    def __init__(self, nlp, skills_db):\n        self.nlp = nlp\n        self.skills_db = skills_db\n        self.token_dist = TOKEN_DIST\n        self.sign = functools.partial(math.copysign, 1)\n        return\n\n    def make_one(self, cluster, len_):\n        a = [1] * len_\n        return [1*(i in cluster) for i, one in enumerate(a)]\n\n    def split_at_values(self, lst, val):\n        return [i for i, x in enumerate(lst) if x != val]\n\n    def grouper(self, iterable, dist):\n        prev = None\n        group = []\n        for item in iterable:\n            if prev == None or item - prev <= dist:\n                group.append(item)\n            else:\n                yield group\n                group = [item]\n            prev = item\n        if group:\n            yield group\n\n    def get_clusters(self, co_oc):\n        clusters = []\n        for i,row in enumerate(co_oc.tolil().rows):\n            if len(row)== 0:\n                continue\n            # divide row into clusters deivided by 0 : 0 occurence where clusters refer to token id\n            # example [2,3,0,5,0,0] -> [0,1,3] -> [[0,1],[3]]\n            # clusts = list(self.grouper(self.split_at_values(row, 0), 1))\n            clusts = list(self.grouper(row, 1))\n                # select token relative cluster via its idx\n               # example i==0 => [[0,1],[3]] => a = [0,1]\n            a = [c for c in clusts if i in c][0]\n            if a not in clusters:\n                clusters.append(a)\n                    \n        # return unique clusters [token id]\n        return clusters\n\n    def get_corpus(self, text, matches):\n        \"\"\"create a corpus matrix which will be used in future computations.\n\n           Parameters\n           ----------\n           matches (list): list of matches generated by sub matchers\n           text (Text): text object\n\n           Returns\n           -------\n\n               corpus : return binary matrix   => (n :skills matched )* (m : tokens in text )  \n                                                1 : skill contains token\n                                                0 : otherwise\n               look_up : return a mapper from skill_ids to its equivalent row index in corpus\n        \"\"\"\n        \n        len_ = len(text)\n        unique_skills = list(set([match['skill_id'] for match in matches]))\n        skill_text_match_bin = [0]*len_\n        match_df = pd.DataFrame(matches)\n        match_df_group = match_df.groupby('skill_id')['doc_node_id']\n        corpus=[]\n        look_up = {}\n        on_inds =[]\n        for idx,gr in enumerate(match_df_group):\n            skill_id =gr[0]\n            g=gr[1]\n            skill_text_match_bin = [0]*len_\n            look_up[idx]=skill_id\n            on_inds = [j for sub in g for j in sub]\n            skill_text_match_bin_updated = [(i in on_inds)*1 for i, _ in enumerate(skill_text_match_bin)]\n\n            corpus.append(skill_text_match_bin_updated)\n        return np.array(corpus), look_up\n\n    def one_gram_sim(self, text_str, skill_str):\n        # transform into sentence\n        text = text_str + ' ' + skill_str\n        tokens = self.nlp(text)\n        token1, token2 = tokens[0], tokens[1]\n        try:\n            vec_similarity = token1.similarity(token2)\n            return vec_similarity\n        except:\n            # try Levenshtein Distance  if words not found in spacy corpus\n            str_distance_similarity = jellyfish.jaro_distance(\n                text_str.lower(), skill_str.lower())\n            return str_distance_similarity\n\n    def compute_w_ratio(self, skill_id, matched_tokens):\n        skill_name = self.skills_db[skill_id]['high_surfce_forms']['full'].split(\n            ' ')\n        skill_len = self.skills_db[skill_id]['skill_len']\n        # favorize the matched tokens uphead\n        late_match_penalty_coef = 0.1\n        token_ids = sum([(1-late_match_penalty_coef*skill_name.index(token))\n                         for token in matched_tokens])\n\n        return token_ids/skill_len\n\n    def retain(self, text_obj, span, skill_id, sk_look, corpus):\n        \"\"\" add doc here  \"\"\"\n        real_id, type_ = sk_look[skill_id].split('_')\n\n        # get skill len\n        len_ = self.skills_db[real_id]['skill_len']\n        # get intersection length of full  skill name  and span tokens\n        len_condition = corpus[skill_id].dot(span)\n\n        # start :to be deleted\n        s_gr = np.array(list(span))*np.array(list(corpus[skill_id]))\n        def condition(x): return x == 1\n\n        s_gr_n = [idx for idx, element in enumerate(\n            s_gr) if condition(element)]\n        # end\n\n        if type_ == 'oneToken':\n            # if skill is n_gram (n>2)\n            score = self.compute_w_ratio(\n                real_id, [text_obj[ind].lemmed for ind in s_gr_n])\n\n        if type_ == 'fullUni':\n            score = 1\n\n        if type_ == 'lowSurf':\n            if len_ > 1:\n\n                score = sum(s_gr)\n\n            else:\n                # if skill is uni_gram (n=1)\n                text_str = ' '.join([str(text_obj[i])\n                                     for i, val in enumerate(s_gr) if val == 1])\n                skill_str = self.skills_db[real_id]['high_surfce_forms']['full']\n\n                score = self.one_gram_sim(text_str, skill_str)\n\n        return {'skill_id': real_id,\n                'doc_node_id':  [i for i, val in enumerate(s_gr) if val == 1],\n                'doc_node_value': ' '.join([str(text_obj[i]) for i, val in enumerate(s_gr) if val == 1]),\n                'type': type_,\n                'score': score,\n                'len': len_condition\n                }\n    # main functions\n\n    def process_n_gram(self, matches, text_obj: Text):\n        \"\"\"apply on conflicted matches to choose which  ones to keep\n\n           Parameters\n           ----------\n           matches (list): list of matches generated by sub matchers\n           text_obj (Text): text object \n\n           Returns\n           -------\n\n               list: return choosen skills with their given words span in the text and thir score  \n\n           \"\"\"\n        if len(matches) == 0:\n            return matches\n\n        text_tokens = text_obj.lemmed(as_list=True)\n        len_ = len(text_tokens)\n\n        corpus, look_up = self.get_corpus(text_tokens, matches)\n        corpus_csr = csr_matrix(corpus)\n        # generate spans (a span is a list of tokens where one or more skills are matched)\n\n        # co-occurence of tokens aij : co-occurence count of token i with token j\n        # co_occ = np.matmul(corpus.T, corpus)\n        co_occ_csr = corpus_csr.T.dot(corpus_csr)\n        # create spans of tokens that co-occured\n        clusters = self.get_clusters(co_occ_csr)\n\n        # one hot encoding of clusters\n        # example [0,1,2] => [1,1,1,0,0,0] , encoding length  = text length\n        ones = [self.make_one(cluster, len_) for cluster in clusters]\n        # generate list of span and list of skills that have conflict on spans [(span,[skill_id])]\n        spans_conflicts = [(np.array(one), np.array([a_[0] for a_ in np.argwhere(corpus_csr.dot(one) != 0)]))\n                           for one in ones]\n\n        # filter and score\n        new_spans = []\n        for span_conflict in spans_conflicts:\n            span, skill_ids = span_conflict\n            span_scored_skills = []\n            types = []\n            scores = []\n            lens = []\n            for sk_id in skill_ids:\n                # score skill given span\n                scored_sk_obj = self.retain(\n                    text_obj, span, sk_id, look_up, corpus)\n                span_scored_skills.append(scored_sk_obj)\n                types.append(scored_sk_obj['type'])\n                lens.append(scored_sk_obj['len'])\n                scores.append(scored_sk_obj['score'])\n            # extract best candiate for a given span\n            if 'oneToken' in types and len(set(types)) > 1:\n                # having a ngram skill with other types in span condiates :\n                # priotize skills with high match length if length >1\n                id_ = np.array(scores).argmax()\n                max_score = 0.5  # selection treshold\n                for i, len_ in enumerate(lens):\n                    if len_ > 1 and types[i] == 'oneToken':\n                        if scores[i] >= max_score:\n                            id_ = i\n\n                new_spans.append(span_scored_skills[id_])\n\n            else:\n                max_score_index = np.array(scores).argmax()\n                new_spans.append(span_scored_skills[max_score_index])\n\n        return new_spans\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py",
        "skillNer\\skill_extractor_class.py/SkillExtractor/__init__"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the object with the provided parameters.\n\n**parameters**:\n- nlp: An object representing natural language processing capabilities.\n- skills_db: A database containing skills information.\n\n**Code Description**:\nIn this function, the provided nlp object and skills_db database are assigned to the respective attributes of the object. Additionally, the token_dist attribute is set to a predefined constant TOKEN_DIST. The sign attribute is set using functools.partial to create a partial function with math.copysign and a fixed value of 1.\n\n**Note**:\nEnsure that the nlp and skills_db parameters are correctly instantiated before calling this function to avoid any errors related to missing attributes.\n\n**Output Example**:\nNo explicit return value is provided in this function."
      ],
      "code_start_line": 18,
      "code_end_line": 23,
      "params": [
        "self",
        "nlp",
        "skills_db"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, nlp, skills_db):\n        self.nlp = nlp\n        self.skills_db = skills_db\n        self.token_dist = TOKEN_DIST\n        self.sign = functools.partial(math.copysign, 1)\n        return\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "make_one",
      "md_content": [
        "**make_one**: The function of make_one is to create a one-hot encoding representation of clusters based on a specified length.\n\n**parameters**:\n- cluster: A list of clusters to be encoded.\n- len_: The length of the encoding.\n\n**Code Description**:\nThe `make_one` function takes a `cluster` and a `len_` parameter. It initializes a list `a` with ones of length `len_`, then generates a one-hot encoding for each element in the cluster by checking if the index is present in the cluster. The function returns a list of one-hot encoded clusters.\n\nIn the context of the project, the `make_one` function is called within the `process_n_gram` function in the `Utils` class. It is used to create one-hot encodings for clusters derived from co-occurrence matrices. These encodings are then utilized to resolve conflicts between skills identified in text spans.\n\n**Note**:\nEnsure that the `cluster` parameter contains valid indices within the specified length `len_`.\n\n**Output Example**:\n[1, 1, 0, 0, 1]"
      ],
      "code_start_line": 25,
      "code_end_line": 27,
      "params": [
        "self",
        "cluster",
        "len_"
      ],
      "have_return": true,
      "code_content": "    def make_one(self, cluster, len_):\n        a = [1] * len_\n        return [1*(i in cluster) for i, one in enumerate(a)]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\utils.py/Utils/process_n_gram"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "split_at_values",
      "md_content": [
        "**split_at_values**: The function of split_at_values is to return the indices of elements in a list that are not equal to a specified value.\n\n**parameters**:\n- lst: The list in which to search for values.\n- val: The value to compare elements against.\n\n**Code Description**:\nThe split_at_values function takes a list (lst) and a value (val) as input parameters. It then iterates over the elements in the list and returns the indices of elements that are not equal to the specified value.\n\n**Note**:\n- This function does not modify the original list, it only returns the indices of elements that meet the condition.\n- If no elements in the list match the specified value, an empty list will be returned.\n\n**Output Example**:\nIf lst = [1, 2, 3, 2, 4] and val = 2, the function will return [0, 2, 4] as the indices of elements that are not equal to 2."
      ],
      "code_start_line": 29,
      "code_end_line": 30,
      "params": [
        "self",
        "lst",
        "val"
      ],
      "have_return": true,
      "code_content": "    def split_at_values(self, lst, val):\n        return [i for i, x in enumerate(lst) if x != val]\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "grouper",
      "md_content": [
        "**grouper**: The function of grouper is to group elements from an iterable based on a specified distance.\n\n**parameters**:\n- iterable: The iterable containing elements to be grouped.\n- dist: The distance used to determine when to start a new group.\n\n**Code Description**:\nThe grouper function takes an iterable and a distance as input parameters. It iterates over the elements in the iterable and groups them based on the specified distance. If the difference between the current element and the previous element is less than or equal to the distance, the element is added to the current group. If the difference exceeds the distance, a new group is started. The function yields each group as a list of elements. If there are any remaining elements in the last group, it yields that group as well.\n\nIn the project, the grouper function is utilized in the get_clusters method of the Utils class. Within get_clusters, the grouper function is used to divide rows of a matrix into clusters based on a specific condition. The clusters are then processed further to extract relevant information.\n\n**Note**:\nEnsure that the input iterable contains elements that can be compared using the specified distance."
      ],
      "code_start_line": 32,
      "code_end_line": 43,
      "params": [
        "self",
        "iterable",
        "dist"
      ],
      "have_return": false,
      "code_content": "    def grouper(self, iterable, dist):\n        prev = None\n        group = []\n        for item in iterable:\n            if prev == None or item - prev <= dist:\n                group.append(item)\n            else:\n                yield group\n                group = [item]\n            prev = item\n        if group:\n            yield group\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\utils.py/Utils/get_clusters"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_clusters",
      "md_content": [
        "**get_clusters**: The function of get_clusters is to extract unique clusters from a matrix based on a specific condition.\n\n**parameters**:\n- co_oc: The matrix containing co-occurrence data.\n\n**Code Description**:\nThe get_clusters function iterates over the rows of the co-occurrence matrix and divides them into clusters based on a condition. It groups elements together until a specific value is encountered, then extracts unique clusters and returns them as a list.\n\nWithin the code, the grouper function is utilized to group elements based on a distance parameter. The clusters extracted from the matrix rows are processed further to identify and store unique clusters. The final list of unique clusters is returned as the output of the function.\n\n**Note**:\nEnsure that the input matrix (co_oc) is in the appropriate format for processing.\nThe function assumes that the input matrix contains relevant data for clustering.\n\n**Output Example**:\n[0, 1, 3]"
      ],
      "code_start_line": 45,
      "code_end_line": 61,
      "params": [
        "self",
        "co_oc"
      ],
      "have_return": true,
      "code_content": "    def get_clusters(self, co_oc):\n        clusters = []\n        for i,row in enumerate(co_oc.tolil().rows):\n            if len(row)== 0:\n                continue\n            # divide row into clusters deivided by 0 : 0 occurence where clusters refer to token id\n            # example [2,3,0,5,0,0] -> [0,1,3] -> [[0,1],[3]]\n            # clusts = list(self.grouper(self.split_at_values(row, 0), 1))\n            clusts = list(self.grouper(row, 1))\n                # select token relative cluster via its idx\n               # example i==0 => [[0,1],[3]] => a = [0,1]\n            a = [c for c in clusts if i in c][0]\n            if a not in clusters:\n                clusters.append(a)\n                    \n        # return unique clusters [token id]\n        return clusters\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\utils.py/Utils/process_n_gram"
      ],
      "reference_who": [
        "skillNer\\utils.py/Utils/grouper"
      ],
      "special_reference_type": [
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "get_corpus",
      "md_content": [
        "**get_corpus**: The function of get_corpus is to create a corpus matrix used for future computations based on matches and text input.\n\n**parameters**:\n- matches (list): List of matches generated by sub matchers.\n- text (Text): Text object.\n\n**Code Description**:\nThe get_corpus function takes a list of matches and a text object as input. It generates a binary matrix corpus where each row represents a skill and each column represents a token in the text. The value 1 in the matrix indicates that a skill contains the token, while 0 indicates otherwise. Additionally, it returns a lookup mapper from skill IDs to their corresponding row indices in the corpus.\n\nIn the calling object process_n_gram, the get_corpus function is utilized to create a corpus matrix from text tokens and matches. The corpus matrix is then converted to a CSR matrix for further computations. The function also involves generating spans of tokens, calculating co-occurrence of tokens, creating clusters of co-occurred tokens, and filtering and scoring skills based on conflicts on spans.\n\n**Note**:\n- The get_corpus function is essential for creating a matrix representation of skills and tokens for subsequent processing in the skillNer module.\n- Ensure that the input matches and text object are correctly formatted to avoid errors in corpus generation.\n\n**Output Example**:\n```\n(array([[1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]]), {0: 'skill_id_1', 1: 'skill_id_2', 2: 'skill_id_3'})\n```"
      ],
      "code_start_line": 63,
      "code_end_line": 97,
      "params": [
        "self",
        "text",
        "matches"
      ],
      "have_return": true,
      "code_content": "    def get_corpus(self, text, matches):\n        \"\"\"create a corpus matrix which will be used in future computations.\n\n           Parameters\n           ----------\n           matches (list): list of matches generated by sub matchers\n           text (Text): text object\n\n           Returns\n           -------\n\n               corpus : return binary matrix   => (n :skills matched )* (m : tokens in text )  \n                                                1 : skill contains token\n                                                0 : otherwise\n               look_up : return a mapper from skill_ids to its equivalent row index in corpus\n        \"\"\"\n        \n        len_ = len(text)\n        unique_skills = list(set([match['skill_id'] for match in matches]))\n        skill_text_match_bin = [0]*len_\n        match_df = pd.DataFrame(matches)\n        match_df_group = match_df.groupby('skill_id')['doc_node_id']\n        corpus=[]\n        look_up = {}\n        on_inds =[]\n        for idx,gr in enumerate(match_df_group):\n            skill_id =gr[0]\n            g=gr[1]\n            skill_text_match_bin = [0]*len_\n            look_up[idx]=skill_id\n            on_inds = [j for sub in g for j in sub]\n            skill_text_match_bin_updated = [(i in on_inds)*1 for i, _ in enumerate(skill_text_match_bin)]\n\n            corpus.append(skill_text_match_bin_updated)\n        return np.array(corpus), look_up\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\utils.py/Utils/process_n_gram"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "one_gram_sim",
      "md_content": [
        "**one_gram_sim**: The function of one_gram_sim is to calculate the similarity between two input strings using word vectors from a natural language processing model.\n\n**parameters**:\n- text_str: The first input text string.\n- skill_str: The second input text string.\n\n**Code Description**:\nThe one_gram_sim function takes two text strings as input, combines them into a single sentence, tokenizes the sentence using a natural language processing model, and then calculates the similarity between the first two tokens of the sentence. If the tokens are not found in the model, it falls back to calculating the similarity using the Jaro distance metric. The function returns the similarity score between the two strings.\n\nIn the calling object \"retain\", the one_gram_sim function is used when the type of a skill is 'lowSurf' and the length of the skill is 1. In this case, the function calculates the similarity between a subset of the input text and the full surface form of the skill. The result is used to determine the score for the skill matching process.\n\n**Note**:\n- Ensure that the input strings are in lowercase for accurate similarity calculations.\n- Handle exceptions if the tokens are not found in the natural language processing model.\n\n**Output Example**:\n{\n    'skill_id': '123',\n    'doc_node_id': [0, 2, 4],\n    'doc_node_value': 'example text here',\n    'type': 'lowSurf',\n    'score': 0.85,\n    'len': 2\n}"
      ],
      "code_start_line": 99,
      "code_end_line": 111,
      "params": [
        "self",
        "text_str",
        "skill_str"
      ],
      "have_return": true,
      "code_content": "    def one_gram_sim(self, text_str, skill_str):\n        # transform into sentence\n        text = text_str + ' ' + skill_str\n        tokens = self.nlp(text)\n        token1, token2 = tokens[0], tokens[1]\n        try:\n            vec_similarity = token1.similarity(token2)\n            return vec_similarity\n        except:\n            # try Levenshtein Distance  if words not found in spacy corpus\n            str_distance_similarity = jellyfish.jaro_distance(\n                text_str.lower(), skill_str.lower())\n            return str_distance_similarity\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\utils.py/Utils/retain"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "compute_w_ratio",
      "md_content": [
        "**compute_w_ratio**: The function of compute_w_ratio is to calculate the ratio of matched tokens to the length of a specific skill.\n\n**parameters**:\n- self: The instance of the class.\n- skill_id: The identifier of the skill to be evaluated.\n- matched_tokens: A list of tokens that match the skill.\n\n**Code Description**:\nThe `compute_w_ratio` function first extracts the full name of the skill and its length from the skills database based on the provided `skill_id`. It then calculates a token matching score by favoring tokens that appear earlier in the skill name. The function applies a penalty coefficient for tokens that appear later in the skill name. Finally, it returns the ratio of the total token matching score to the length of the skill.\n\nIn the calling function `retain`, the `compute_w_ratio` function is used to calculate the similarity score for different types of skills based on the matched tokens. Depending on the type of skill (oneToken, fullUni, lowSurf), different calculations are performed using the `compute_w_ratio` function. The function plays a crucial role in determining the similarity score between the text span and the skill based on the matched tokens.\n\n**Note**:\nIt is essential to ensure that the `skill_id` provided to the `compute_w_ratio` function corresponds to a valid skill in the skills database to avoid errors.\n\n**Output Example**:\n{\n    'skill_id': 1234,\n    'doc_node_id': [0, 2, 4],\n    'doc_node_value': 'example text',\n    'type': 'oneToken',\n    'score': 0.75,\n    'len': 2\n}"
      ],
      "code_start_line": 113,
      "code_end_line": 122,
      "params": [
        "self",
        "skill_id",
        "matched_tokens"
      ],
      "have_return": true,
      "code_content": "    def compute_w_ratio(self, skill_id, matched_tokens):\n        skill_name = self.skills_db[skill_id]['high_surfce_forms']['full'].split(\n            ' ')\n        skill_len = self.skills_db[skill_id]['skill_len']\n        # favorize the matched tokens uphead\n        late_match_penalty_coef = 0.1\n        token_ids = sum([(1-late_match_penalty_coef*skill_name.index(token))\n                         for token in matched_tokens])\n\n        return token_ids/skill_len\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\utils.py/Utils/retain"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "retain",
      "md_content": [
        "**retain**: The function of retain is to calculate the similarity score between a given text span and a specific skill based on various conditions and types of skills.\n\n**parameters**:\n- text_obj: The text object containing the text span.\n- span: The span of tokens representing the text span.\n- skill_id: The identifier of the skill to be evaluated.\n- sk_look: A dictionary containing information about the skill.\n- corpus: The corpus of skills and tokens.\n\n**Code Description**:\nThe retain function first extracts the real_id and type of the skill based on the provided skill_id. It then performs calculations based on the type of the skill ('oneToken', 'fullUni', 'lowSurf') to determine the similarity score. The function utilizes other functions such as compute_w_ratio and one_gram_sim to calculate scores for different skill types. The output includes the skill_id, the index of tokens in the text span, the text values of those tokens, the type of skill, the calculated score, and the length condition.\n\nIn the calling function process_n_gram, the retain function is used to score skills based on conflicts in spans of tokens. It iterates through conflicting spans, calculates scores for each skill within the span, and selects the best candidate skill based on predefined conditions. The retain function plays a crucial role in determining the best-matched skill for a given text span in the context of skill extraction and matching.\n\n**Note**:\n- Ensure that the input parameters are correctly provided to avoid errors in score calculations.\n- Understand the different types of skills ('oneToken', 'fullUni', 'lowSurf') and their impact on the scoring mechanism.\n- Handle exceptions that may arise during the similarity score calculations.\n\n**Output Example**:\n{\n    'skill_id': '123',\n    'doc_node_id': [0, 2, 4],\n    'doc_node_value': 'example text here',\n    'type': 'lowSurf',\n    'score': 0.85,\n    'len': 2\n}"
      ],
      "code_start_line": 124,
      "code_end_line": 168,
      "params": [
        "self",
        "text_obj",
        "span",
        "skill_id",
        "sk_look",
        "corpus"
      ],
      "have_return": true,
      "code_content": "    def retain(self, text_obj, span, skill_id, sk_look, corpus):\n        \"\"\" add doc here  \"\"\"\n        real_id, type_ = sk_look[skill_id].split('_')\n\n        # get skill len\n        len_ = self.skills_db[real_id]['skill_len']\n        # get intersection length of full  skill name  and span tokens\n        len_condition = corpus[skill_id].dot(span)\n\n        # start :to be deleted\n        s_gr = np.array(list(span))*np.array(list(corpus[skill_id]))\n        def condition(x): return x == 1\n\n        s_gr_n = [idx for idx, element in enumerate(\n            s_gr) if condition(element)]\n        # end\n\n        if type_ == 'oneToken':\n            # if skill is n_gram (n>2)\n            score = self.compute_w_ratio(\n                real_id, [text_obj[ind].lemmed for ind in s_gr_n])\n\n        if type_ == 'fullUni':\n            score = 1\n\n        if type_ == 'lowSurf':\n            if len_ > 1:\n\n                score = sum(s_gr)\n\n            else:\n                # if skill is uni_gram (n=1)\n                text_str = ' '.join([str(text_obj[i])\n                                     for i, val in enumerate(s_gr) if val == 1])\n                skill_str = self.skills_db[real_id]['high_surfce_forms']['full']\n\n                score = self.one_gram_sim(text_str, skill_str)\n\n        return {'skill_id': real_id,\n                'doc_node_id':  [i for i, val in enumerate(s_gr) if val == 1],\n                'doc_node_value': ' '.join([str(text_obj[i]) for i, val in enumerate(s_gr) if val == 1]),\n                'type': type_,\n                'score': score,\n                'len': len_condition\n                }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\utils.py/Utils/process_n_gram"
      ],
      "reference_who": [
        "skillNer\\utils.py/Utils/one_gram_sim",
        "skillNer\\utils.py/Utils/compute_w_ratio"
      ],
      "special_reference_type": [
        false,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "condition",
      "md_content": [
        "**condition**: The function of condition is to check if the input x is equal to 1.\n**parameters**:\n- x: The input value to be checked for equality with 1.\n**Code Description**: The condition function takes a single parameter x and returns True if x is equal to 1, otherwise it returns False.\n**Note**: This function is a simple equality check and can be used to evaluate whether a given value is equal to 1.\n**Output Example**: \n- condition(1) \nOutput: True"
      ],
      "code_start_line": 135,
      "code_end_line": 135,
      "params": [
        "x"
      ],
      "have_return": true,
      "code_content": "        def condition(x): return x == 1\n",
      "name_column": 12,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "process_n_gram",
      "md_content": [
        "**process_n_gram**: The process_n_gram function is responsible for resolving conflicts between skills identified in text spans and selecting the best-matched skills based on specific conditions.\n\n**parameters**:\n- matches (list): A list of matches generated by sub matchers.\n- text_obj (Text): The text object representing the input text.\n\n**Code Description**:\nThe process_n_gram function takes a list of matches and a text object as input. It first checks if there are any matches present. If there are no matches, it returns an empty list. Otherwise, it proceeds with further processing.\n\nThe function starts by extracting the lemmatized version of the text tokens using the lemmed method of the text_obj. It then calculates the length of the text_tokens.\n\nNext, the function calls the get_corpus method to generate a binary corpus matrix and a lookup mapper. The corpus matrix represents the presence or absence of tokens in each skill, while the lookup mapper maps skill IDs to their corresponding row indices in the corpus matrix.\n\nThe corpus matrix is then converted to a CSR matrix using the csr_matrix function from the scipy.sparse module. This conversion is necessary for efficient matrix operations.\n\nThe function proceeds to calculate the co-occurrence of tokens in the corpus matrix by performing matrix multiplication. The resulting co-occurrence matrix is stored in the co_occ_csr variable.\n\nThe get_clusters method is called to extract unique clusters from the co-occurrence matrix. This is done by iterating over the rows of the co-occurrence matrix and dividing them into clusters based on a specific condition. The clusters are stored in the clusters variable.\n\nThe make_one method is called to generate a one-hot encoding representation of the clusters. This is done by creating a list of ones with a length equal to the number of tokens in the text, and then generating a one-hot encoding for each cluster by checking if the index is present in the cluster. The resulting one-hot encoded clusters are stored in the ones variable.\n\nThe spans_conflicts variable is initialized as an empty list. The function then iterates over each one-hot encoded cluster and generates a list of spans and a list of skill IDs that have conflicts on those spans. Each span is represented as a numpy array, and the skill IDs are represented as a numpy array of the corresponding row indices in the corpus matrix. The spans_conflicts list is populated with tuples containing the span and skill ID arrays.\n\nThe function then filters and scores the spans_conflicts based on predefined conditions. It initializes an empty list new_spans to store the selected skills. For each span_conflict in spans_conflicts, it extracts the span and skill IDs. It then iterates over each skill ID and scores the skill given the span using the retain method. The scored skill objects are stored in the span_scored_skills list.\n\nThe function checks if there is a conflict between a oneToken skill and other types of skills in the span candidates. If such a conflict exists and the length of the span is greater than 1, it prioritizes skills with a high match length. Otherwise, it selects the skill with the highest score.\n\nThe selected skill objects are appended to the new_spans list. Finally, the new_spans list is returned as the output of the function.\n\n**Note**:\n- The process_n_gram function is an important step in the skill extraction process as it resolves conflicts between skills and selects the most appropriate skills based on specific conditions.\n- Ensure that the input matches and text object are correctly formatted to avoid errors in the processing.\n- Understand the different types of skills and their impact on the selection mechanism.\n- The function utilizes other methods such as get_corpus, get_clusters, make_one, and retain to perform various operations.\n- The output of the function is a list of selected skills with their corresponding information.\n\n**Output Example**:\n```python\n[\n    {\n        'skill_id': '123',\n        'doc_node_id': [0, 2, 4],\n        'doc_node_value': 'example text here',\n        'type': 'lowSurf',\n        'score': 0.85,\n        'len': 2\n    },\n    {\n        'skill_id': '456',\n        'doc_node_id': [1, 3, 5],\n        'doc_node_value': 'another example',\n        'type': 'oneToken',\n        'score': 0.92,\n        'len': 3\n    },\n    ...\n]\n```"
      ],
      "code_start_line": 171,
      "code_end_line": 241,
      "params": [
        "self",
        "matches",
        "text_obj"
      ],
      "have_return": true,
      "code_content": "    def process_n_gram(self, matches, text_obj: Text):\n        \"\"\"apply on conflicted matches to choose which  ones to keep\n\n           Parameters\n           ----------\n           matches (list): list of matches generated by sub matchers\n           text_obj (Text): text object \n\n           Returns\n           -------\n\n               list: return choosen skills with their given words span in the text and thir score  \n\n           \"\"\"\n        if len(matches) == 0:\n            return matches\n\n        text_tokens = text_obj.lemmed(as_list=True)\n        len_ = len(text_tokens)\n\n        corpus, look_up = self.get_corpus(text_tokens, matches)\n        corpus_csr = csr_matrix(corpus)\n        # generate spans (a span is a list of tokens where one or more skills are matched)\n\n        # co-occurence of tokens aij : co-occurence count of token i with token j\n        # co_occ = np.matmul(corpus.T, corpus)\n        co_occ_csr = corpus_csr.T.dot(corpus_csr)\n        # create spans of tokens that co-occured\n        clusters = self.get_clusters(co_occ_csr)\n\n        # one hot encoding of clusters\n        # example [0,1,2] => [1,1,1,0,0,0] , encoding length  = text length\n        ones = [self.make_one(cluster, len_) for cluster in clusters]\n        # generate list of span and list of skills that have conflict on spans [(span,[skill_id])]\n        spans_conflicts = [(np.array(one), np.array([a_[0] for a_ in np.argwhere(corpus_csr.dot(one) != 0)]))\n                           for one in ones]\n\n        # filter and score\n        new_spans = []\n        for span_conflict in spans_conflicts:\n            span, skill_ids = span_conflict\n            span_scored_skills = []\n            types = []\n            scores = []\n            lens = []\n            for sk_id in skill_ids:\n                # score skill given span\n                scored_sk_obj = self.retain(\n                    text_obj, span, sk_id, look_up, corpus)\n                span_scored_skills.append(scored_sk_obj)\n                types.append(scored_sk_obj['type'])\n                lens.append(scored_sk_obj['len'])\n                scores.append(scored_sk_obj['score'])\n            # extract best candiate for a given span\n            if 'oneToken' in types and len(set(types)) > 1:\n                # having a ngram skill with other types in span condiates :\n                # priotize skills with high match length if length >1\n                id_ = np.array(scores).argmax()\n                max_score = 0.5  # selection treshold\n                for i, len_ in enumerate(lens):\n                    if len_ > 1 and types[i] == 'oneToken':\n                        if scores[i] >= max_score:\n                            id_ = i\n\n                new_spans.append(span_scored_skills[id_])\n\n            else:\n                max_score_index = np.array(scores).argmax()\n                new_spans.append(span_scored_skills[max_score_index])\n\n        return new_spans\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/annotate"
      ],
      "reference_who": [
        "skillNer\\text_class.py/Text",
        "skillNer\\text_class.py/Text/lemmed",
        "skillNer\\utils.py/Utils/make_one",
        "skillNer\\utils.py/Utils/get_clusters",
        "skillNer\\utils.py/Utils/get_corpus",
        "skillNer\\utils.py/Utils/retain"
      ],
      "special_reference_type": [
        true,
        false,
        false,
        false,
        false,
        false
      ]
    }
  ],
  "skillNer\\network\\remote_db.py": [
    {
      "type": "ClassDef",
      "name": "RemoteBucket",
      "md_content": [
        "**RemoteBucket**: The function of RemoteBucket is to fetch databases (db) from a remote repository stored in `.json` files.\n\n**attributes**:\n- token: Your GitHub token for accessing a private repository (optional, default is an empty string).\n- branch: The branch from which to fetch the database (optional, default is \"master\").\n\n**Code Description**:\nThe RemoteBucket class is designed to fetch databases from a remote repository. The constructor initializes the class with a GitHub token and branch information. The `fetch_remote` method is used to retrieve a specific database by providing its name. The method constructs an endpoint URL based on the branch and database name, makes a request to the remote repository, and returns the fetched database in the form of a Python dictionary.\n\nThe class utilizes the `requests` library to send HTTP requests to the remote repository. If a GitHub token is provided, it is included in the request headers for accessing private repositories. The fetched database is returned in JSON format.\n\nIn the project structure, the RemoteBucket class is located in the `remote_db.py` file under the `network` module. It is imported and used in other parts of the project, such as in the `general_params.py` file, where databases are fetched by creating an instance of RemoteBucket and calling the `fetch_remote` method with the desired database name.\n\n**Note**:\nDevelopers using the RemoteBucket class should ensure they have the necessary permissions to access the remote repository if it is private. Additionally, they should provide the correct database name when calling the `fetch_remote` method.\n\n**Output Example**:\n```python\n{\n    \"example_key\": \"example_value\",\n    \"nested_dict\": {\n        \"nested_key\": \"nested_value\"\n    },\n    ...\n}\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 81,
      "params": [],
      "have_return": true,
      "code_content": "class RemoteBucket:\n    \"\"\"Main class to fetch data bases (db) from repo. db are saved in a `.json` files\n    \"\"\"\n\n    def __init__(\n        self,\n        token: str = \"\",\n        branch: str = \"master\"\n    ) -> None:\n        \"\"\"Constructor of the class\n\n        Parameters\n        ----------\n        token : str, optional\n            Your GitHub token in case repo is private, by default \"\" which is the case of public repo\n        branch : str, optional\n            the branch from which to fetch db, by default \"master\"\n        \"\"\"\n\n        # save params\n        self.token = token\n        self.branch = branch\n\n        # construct endpoint\n        self.end_point = f\"https://raw.githubusercontent.com/AnasAito/SkillNER/{self.branch}\"\n        return\n\n    def fetch_remote(\n        self,\n        db_name: str\n    ) -> dict:\n        \"\"\"Function to fetch db\n\n        Parameters\n        ----------\n        db_name : str in [\"SKILL_DB\", \"TOKEN_DIST\"]\n            Name of the db to fetch\n\n        Returns\n        -------\n        dict\n            returns the db in format of a python dict object\n\n        Examples\n        --------\n        >>> from skillNer.network.remote_db import RemoteBucket\n        >>> buckets = RemoteBucket(\n            branch=\"master\"\n        )\n        >>> buckets.fetch_remote(\"SKILL_DB\")\n        ...\n        \"\"\"\n        # request props\n        url = f\"{self.end_point}/{MAPPING_NAME_URL[db_name]}\"\n\n        # check if repo is private\n        if self.token:\n            headers = {\n                'Authorization': f'token {self.token}'\n            }\n        else:\n            headers = {}\n\n        # fetch\n        response = requests.get(\n            url=url,\n            headers=headers\n        )\n\n        # return content in json format\n        return response.json()\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\general_params.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the RemoteBucket object with the provided GitHub token and branch information.\n\n**parameters**:\n- token: str, optional\n  Your GitHub token in case the repository is private. Default value is an empty string for public repositories.\n- branch: str, optional\n  The branch from which to fetch the database. Default value is \"master\".\n\n**Code Description**:\nThe __init__ function serves as the constructor for the RemoteBucket class. It takes in two optional parameters: token, which represents the GitHub token for private repositories, and branch, which specifies the branch to fetch the database from. The function saves the provided parameters as attributes (self.token and self.branch) and constructs the endpoint URL based on the branch information.\n\n**Note**:\nEnsure the GitHub token is provided if accessing a private repository. The default values are set for public repositories.\n\n**Output Example**:\nIf token is provided as \"abc123\" and branch is set to \"development\", the endpoint URL will be \"https://raw.githubusercontent.com/AnasAito/SkillNER/development\"."
      ],
      "code_start_line": 15,
      "code_end_line": 36,
      "params": [
        "self",
        "token",
        "branch"
      ],
      "have_return": true,
      "code_content": "    def __init__(\n        self,\n        token: str = \"\",\n        branch: str = \"master\"\n    ) -> None:\n        \"\"\"Constructor of the class\n\n        Parameters\n        ----------\n        token : str, optional\n            Your GitHub token in case repo is private, by default \"\" which is the case of public repo\n        branch : str, optional\n            the branch from which to fetch db, by default \"master\"\n        \"\"\"\n\n        # save params\n        self.token = token\n        self.branch = branch\n\n        # construct endpoint\n        self.end_point = f\"https://raw.githubusercontent.com/AnasAito/SkillNER/{self.branch}\"\n        return\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "fetch_remote",
      "md_content": [
        "**fetch_remote**: The function of fetch_remote is to fetch a specific database from a remote server and return it in the format of a Python dictionary.\n\n**parameters**:\n- db_name: Name of the database to fetch, which should be either \"SKILL_DB\" or \"TOKEN_DIST\".\n\n**Code Description**:\nThe fetch_remote function constructs a URL based on the provided db_name parameter and sends a GET request to the remote server to fetch the database. It handles private repositories by including an authorization token in the request headers if available. The function then returns the content of the response in JSON format as a Python dictionary.\n\nThis function is a part of the RemoteBucket class in the remote_db.py module under the network package. It is designed to work with instances of RemoteBucket to fetch specific databases from a remote server. \n\nWhen called, the fetch_remote function requires the db_name parameter to specify which database to fetch. It is essential to ensure that the db_name parameter is either \"SKILL_DB\" or \"TOKEN_DIST\" to fetch the correct database.\n\n**Note**:\nMake sure to handle any potential exceptions that may occur during the network request, such as connection errors or invalid responses.\n\n**Output Example**:\n```python\n{\n    \"data\": {\n        \"key1\": \"value1\",\n        \"key2\": \"value2\",\n        ...\n    }\n}\n```"
      ],
      "code_start_line": 38,
      "code_end_line": 81,
      "params": [
        "self",
        "db_name"
      ],
      "have_return": true,
      "code_content": "    def fetch_remote(\n        self,\n        db_name: str\n    ) -> dict:\n        \"\"\"Function to fetch db\n\n        Parameters\n        ----------\n        db_name : str in [\"SKILL_DB\", \"TOKEN_DIST\"]\n            Name of the db to fetch\n\n        Returns\n        -------\n        dict\n            returns the db in format of a python dict object\n\n        Examples\n        --------\n        >>> from skillNer.network.remote_db import RemoteBucket\n        >>> buckets = RemoteBucket(\n            branch=\"master\"\n        )\n        >>> buckets.fetch_remote(\"SKILL_DB\")\n        ...\n        \"\"\"\n        # request props\n        url = f\"{self.end_point}/{MAPPING_NAME_URL[db_name]}\"\n\n        # check if repo is private\n        if self.token:\n            headers = {\n                'Authorization': f'token {self.token}'\n            }\n        else:\n            headers = {}\n\n        # fetch\n        response = requests.get(\n            url=url,\n            headers=headers\n        )\n\n        # return content in json format\n        return response.json()\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\general_params.py"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "skillNer\\visualizer\\html_elements.py": [
    {
      "type": "FunctionDef",
      "name": "element",
      "md_content": [
        "**element**: The function of element is to generate an HTML element with specified attributes and content.\n\n**parameters**:\n- ele_type: a string representing the type of HTML element (default is \"div\").\n- className: a string representing the CSS class of the element.\n- children: a list of strings representing the content inside the element.\n- **kwargs: additional key-value pairs to be included as attributes in the element.\n\n**Code Description**:\nThe `element` function constructs an HTML element based on the provided parameters. It iterates over any additional key-value pairs (**kwargs**) to include them as attributes in the element. The function then generates the content of the element by combining the element type, class, additional attributes, and children content into an HTML string.\n\nIn the calling context within the project, the `element` function is utilized in the `render_phrase` function to create various HTML elements for rendering phrases, including meta data components and script elements. The `render_phrase` function handles the display of skill-related information and styling based on the provided phrase object.\n\n**Note**: When using the `element` function, ensure that the parameters are correctly provided to generate the desired HTML element structure.\n\n**Output Example**:\n```html\n<div class='flex grid-cols-2 gap-2 mb-4'>\n    <span class='font-bold col-1'>Key</span>\n    <span class='col-1'>Value</span>\n</div>\n```"
      ],
      "code_start_line": 11,
      "code_end_line": 30,
      "params": [
        "ele_type",
        "className",
        "children"
      ],
      "have_return": true,
      "code_content": "def element(\n    ele_type: str = \"div\",\n    className: str = \"\",\n    children: List = [],\n    **kwargs\n) -> str:\n\n    # addition props\n    other_props = \"\"\n    for key, val in kwargs.items():\n        other_props += f\"{key}='{val}'\"\n\n    # content of the element\n    content = f\"\"\"\n        <{ele_type} class='{className}' {other_props}>\n            {\" \".join(children)}\n        </{ele_type}>\n    \"\"\"\n\n    return content\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\visualizer\\html_elements.py/render_phrase/meta_data_component",
        "skillNer\\visualizer\\html_elements.py/render_phrase"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "render_phrase",
      "md_content": [
        "**render_phrase**: The function of render_phrase is to generate an HTML representation of a Phrase object, including the skill phrase, skill type, and metadata components.\n\n**parameters**:\n- phrase: A Phrase object representing a skill or non-skill phrase.\n\n**Code Description**:\nThe `render_phrase` function takes a `Phrase` object as input and generates an HTML representation of the phrase. If the phrase is a non-skill phrase, it simply returns the raw text surrounded by non-breaking spaces. If the phrase is a skill phrase, it generates an HTML element with the following structure:\n\n- A span element with the class and styling based on the skill type.\n- The raw text of the phrase.\n- A span element with the class for displaying the skill type.\n- A div element for displaying the metadata of the skill phrase.\n- A script element for handling mouse events to show/hide the metadata.\n\nThe function first checks if the phrase is a skill phrase by checking the `is_skill` attribute. If it is not a skill phrase, it returns the raw text surrounded by non-breaking spaces.\n\nFor skill phrases, the function generates a unique ID for the element by combining the skill ID with a random number. It then creates a script element with JavaScript functions to show/hide the metadata based on mouse events. The `on_mouse_enter` and `on_mouse_leave` variables store the function calls for the respective events.\n\nThe function also defines a nested function `meta_data_component` to generate the HTML structure for displaying the metadata. This function takes a key-value pair and returns a div element with two spans for the key and value.\n\nThe `color` variable is assigned the color class based on the skill type using the `SKILL_TO_COLOR_TAILWIND` dictionary.\n\nFinally, the function uses the `element` function (from the `html_elements.py` module) to generate the HTML structure for the skill phrase. It sets the `onmouseleave` and `onmouseenter` attributes to the respective function calls, sets the class based on the skill type, and includes the raw text, skill type, metadata div, and script element as children.\n\n**Note**: When using the `render_phrase` function, ensure that the `Phrase` object has valid values for the skill-related attributes (skill_id, skill_name, skill_type) and that the `element` function is correctly implemented to generate the desired HTML structure.\n\n**Output Example**:\n```html\n<span class='relative p-1 text-white rounded-md border bg-blue-500' onmouseleave='mouseLeaveHandler_123()' onmouseenter='mouseEnterHandler_123()'>\n    Skill Phrase\n    <span class='text-xs text-white font-bold'>(Certification)</span>\n    <div id='123' style='display: none;' class='absolute shadow-lg z-40 bg-white flex-col text-sm text-black p-2 border left-0 -bottom-15'>\n        <div class='flex grid-cols-2 gap-2 mb-4'>\n            <span class='font-bold col-1'>Skill Name</span>\n            <span class='col-1'>Python Programming</span>\n        </div>\n        <div class='flex grid-cols-2 gap-2 mb-4'>\n            <span class='font-bold col-1'>Matching Type</span>\n            <span class='col-1'>Exact Match</span>\n        </div>\n        <div class='flex grid-cols-2 gap-2 mb-4'>\n            <span class='font-bold col-1'>Score</span>\n            <span class='col-1'>0.85</span>\n        </div>\n    </div>\n    <script>\n        function mouseEnterHandler_123() {\n            document.getElementById(\"123\").style.display = \"\";\n        }\n\n        function mouseLeaveHandler_123() {\n            document.getElementById(\"123\").style.display = \"none\";\n        }\n    </script>\n</span>\n```"
      ],
      "code_start_line": 33,
      "code_end_line": 86,
      "params": [
        "phrase"
      ],
      "have_return": true,
      "code_content": "def render_phrase(phrase: Phrase) -> str:\n    # case of non skill phrase\n    if not phrase.is_skill:\n        return \"&nbsp\" + phrase.raw_text + \"&nbsp\"\n\n    # case of skill\n    # give an id to identiify element\n    id_element = f\"{phrase.skill_id}_{random.randint(0, 1000)}\"\n\n    # script to show hide meta data\n    src = \"\"\"\n        function mouseEnterHandler_%s() {\n            document.getElementById(\"%s\").style.display = \"\";\n        }\n\n        function mouseLeaveHandler_%s() {\n            document.getElementById(\"%s\").style.display = \"none\";\n        }\n    \"\"\" % (id_element, id_element, id_element, id_element)\n\n    # props of the div\n    on_mouse_enter = f\"mouseEnterHandler_{id_element}()\"\n    on_mouse_leave = f\"mouseLeaveHandler_{id_element}()\"\n\n    # component for meta data\n    def meta_data_component(key, value): return element(ele_type=\"div\", className=\"flex grid-cols-2 gap-2 mb-4\", children=[\n        element(ele_type=\"span\",\n                className=\"font-bold col-1\", children=[key]),\n        element(ele_type=\"span\", className=\"col-1\", children=[str(value)])\n    ])\n\n    # color of element\n    color = SKILL_TO_COLOR_TAILWIND[phrase.skill_type]\n\n    return element(ele_type=\"span\", onmouseleave=on_mouse_leave, onmouseenter=on_mouse_enter, className=f\"relative p-1 text-white rounded-md border bg-{color}\", children=[\n        # phrase\n        phrase.raw_text,\n\n        # type skill\n        element(ele_type=\"span\", className='text-xs text-white font-bold', children=[\n            \" (\", phrase.skill_type, \")\"\n        ]),\n\n        # meta data\n        element(ele_type=\"div\", id=id_element,\n                style=\"display: none;\",\n                className='absolute shadow-lg z-40 bg-white flex-col text-sm text-black p-2 border left-0 -bottom-15',\n                children=[\n                    meta_data_component(key, val)\n                          for key, val in phrase.get_meta_data().items()\n                ]),\n\n        element(ele_type=\"script\", children=[src])\n    ])\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py",
        "skillNer\\skill_extractor_class.py/SkillExtractor/describe"
      ],
      "reference_who": [
        "skillNer\\visualizer\\html_elements.py/element",
        "skillNer\\visualizer\\phrase_class.py/Phrase",
        "skillNer\\visualizer\\phrase_class.py/Phrase/get_meta_data"
      ],
      "special_reference_type": [
        false,
        true,
        false
      ]
    },
    {
      "type": "FunctionDef",
      "name": "meta_data_component",
      "md_content": [
        "**meta_data_component**: The function of meta_data_component is to generate a metadata component in HTML format containing key-value pairs.\n\n**parameters**:\n- key: A string representing the key of the metadata.\n- value: A value associated with the key in the metadata.\n\n**Code Description**:\nThe `meta_data_component` function creates a metadata component in HTML format. It utilizes the `element` function to generate a `<div>` element with two `<span>` children elements. The first `<span>` element displays the key in a bold format, while the second `<span>` element displays the corresponding value as a string.\n\nThe function structure ensures that the metadata component is structured with the key and value pairs displayed in a visually appealing manner. By using the `element` function, the metadata component is encapsulated within a `<div>` element with specific CSS classes for styling.\n\nIn the context of the project, `meta_data_component` is likely used to display key information related to a specific skill or entity, along with their corresponding values. This function contributes to the overall visual representation of skill-related data within the HTML rendering process.\n\n**Note**: When calling the `meta_data_component` function, provide the key and value parameters to generate the desired metadata component accurately.\n\n**Output Example**:\n```html\n<div class='flex grid-cols-2 gap-2 mb-4'>\n    <span class='font-bold col-1'>Key</span>\n    <span class='col-1'>Value</span>\n</div>\n```"
      ],
      "code_start_line": 58,
      "code_end_line": 62,
      "params": [
        "key",
        "value"
      ],
      "have_return": true,
      "code_content": "    def meta_data_component(key, value): return element(ele_type=\"div\", className=\"flex grid-cols-2 gap-2 mb-4\", children=[\n        element(ele_type=\"span\",\n                className=\"font-bold col-1\", children=[key]),\n        element(ele_type=\"span\", className=\"col-1\", children=[str(value)])\n    ])\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [
        "skillNer\\visualizer\\html_elements.py/element"
      ],
      "special_reference_type": [
        true
      ]
    },
    {
      "type": "FunctionDef",
      "name": "DOM",
      "md_content": [
        "**DOM**: The function of DOM is to generate a HTML document structure with specified children elements.\n\n**parameters**:\n- children: A list of strings representing the children elements to be included in the generated HTML document.\n\n**Code Description**:\nThe DOM function constructs an HTML document with a head section containing a link to an external CSS file and a body section with a root div element that includes the specified children elements. The function then returns the HTML content using the HTML function.\n\nIn the project, the DOM function is utilized within the describe method of the SkillExtractor class in the skill_extractor_class.py file. In this context, the DOM function is used to create the necessary HTML structure for rendering annotated skills by generating phrases and rendering them using the render_phrase function.\n\n**Note**: Ensure that the children parameter passed to the DOM function is a list of strings representing valid HTML elements or content.\n\n**Output Example**:\n```html\n<head>\n    <link\n        id=\"external-css\"\n        rel=\"stylesheet\"\n        type=\"text/css\"\n        href=\"https://unpkg.com/tailwindcss@^2/dist/tailwind.min.css\"\n        media=\"all\"\n    />\n</head>\n<body>\n    <div id=\"root\" class=\"px-4 leading-10 mb-24\">\n        <p>Annotated skill 1</p>\n        <p>Annotated skill 2</p>\n        <p>Annotated skill 3</p>\n    </div>\n</body>\n```"
      ],
      "code_start_line": 89,
      "code_end_line": 108,
      "params": [
        "children"
      ],
      "have_return": true,
      "code_content": "def DOM(children: List[str] = []):\n\n    content = f\"\"\"\n        <head>\n            <link\n                id=\"external-css\"\n                rel=\"stylesheet\"\n                type=\"text/css\"\n                href=\"https://unpkg.com/tailwindcss@^2/dist/tailwind.min.css\"\n                media=\"all\"\n            />\n        </head>\n\n        <body>\n            <div id=\"root\" class=\"px-4 leading-10 mb-24\">\n                {\" \".join(children)}\n            </div>\n        </body>\n    \"\"\"\n    return HTML(content)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py",
        "skillNer\\skill_extractor_class.py/SkillExtractor/describe"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "skillNer\\visualizer\\phrase_class.py": [
    {
      "type": "ClassDef",
      "name": "Phrase",
      "md_content": [
        "**Phrase**: The function of Phrase is to represent a data structure for building HTML visualization of annotated text, distinguishing between skill and non-skill phrases.\n\n**attributes**:\n- raw_text: stores the raw input text.\n- start: the starting position of the phrase in the original text.\n- end: the ending position of the phrase in the original text.\n- is_skill: a boolean indicating if the phrase is a skill phrase.\n- skill_id: the ID of the skill associated with the phrase.\n- skill_name: the name of the skill.\n- skill_type: the type of skill (certification, hard skill, or soft skill).\n- score: the score associated with the skill.\n- type_matching: the type of matching for the skill phrase (full match, n-gram).\n\n**Code Description**:\nThe `Phrase` class serves as a data structure to handle annotated text visualization. It contains attributes to store information about the text, such as the raw text, position in the original text, skill type, and metadata related to the skill. The `get_meta_data` method returns a dictionary of relevant metadata for the phrase. The `split_text_to_phrase` static method is the main function that splits the text into skill and non-skill phrases based on the annotation results and a skill database. It creates `Phrase` objects for skill phrases, populates their attributes, and organizes them along with non-skill phrases to form a list of phrases.\n\nThe `render_phrase` function in the `html_elements.py` module is called to render the HTML representation of each `Phrase` object. It handles the display of skill and non-skill phrases, including showing/hiding metadata on mouse events and styling elements based on the skill type.\n\n**Note**:\nDevelopers can utilize the `Phrase` class to manage and visualize annotated text, distinguishing between skill and non-skill phrases effectively.\n\n**Output Example**:\nA possible output could be a visually structured HTML representation of annotated text with skill phrases highlighted and metadata displayed upon interaction."
      ],
      "code_start_line": 5,
      "code_end_line": 136,
      "params": [],
      "have_return": true,
      "code_content": "class Phrase:\n    \"\"\"Data structure to build html visualization of the annotated text.\n    The input text is split into skill phrase and non skill phrase\n    \"\"\"\n\n    def __init__(\n        self,\n        text: str\n    ) -> None:\n\n        # save raw text\n        self.raw_text = text\n\n        # position of phrase in original text\n        self.start: int = None\n        self.end: int = None\n\n        # type of phrase: skill or not\n        self.is_skill: bool = False\n\n        # meta data\n        self.skill_id: str = \"\"\n        self.skill_name: str = \"\"\n        self.skill_type: str = \"\"  # certification, hard skill or soft skill\n        self.score: float = None\n        self.type_matching: str = \"\"  # full match, n_gram\n\n    def get_meta_data(self) -> dict:\n\n        return {\n            \"skill name\": self.skill_name,\n            \"matching type\": self.type_matching,\n            \"score\": self.score\n        }\n\n    @staticmethod\n    def split_text_to_phare(\n        annotation: dict,  # result of skill extractor\n        SKILL_DB: dict\n    ) -> List:\n        \"\"\"Main function to distinguish skill and non skill phrases\n\n        Parameters\n        ----------\n        annotation : dict\n            The output of the ``SkillExtractor.annotate``\n        SKILL_DB: dict\n            Data base of skills\n\n        Returns\n        -------\n        List\n            returns a list of phrases\n        \"\"\"\n        # params\n        list_words = annotation[\"text\"].split(\" \")\n\n        # find skill phrases\n        arr_skill_phrases = []\n\n        for type_matching, arr_skills in annotation[\"results\"].items():\n            for skill in arr_skills:\n                # create a phrase object\n                phrase = Phrase(text=skill[\"doc_node_value\"])\n                phrase.is_skill = True\n\n                # index word start and end\n                start = skill[\"doc_node_id\"][0]\n                end = skill[\"doc_node_id\"][-1]\n\n                phrase.start = start\n                phrase.end = end\n\n                # meta data\n                phrase.type_matching = type_matching\n                phrase.skill_id = skill[\"skill_id\"]\n                phrase.skill_name = SKILL_DB[skill[\"skill_id\"]][\"skill_name\"]\n                phrase.skill_type = SKILL_DB[skill[\"skill_id\"]][\"skill_type\"]\n                phrase.score = skill[\"score\"]\n\n                # append phrase\n                arr_skill_phrases.append(phrase)\n\n        # handle case where no skill was annotated in text\n        if not len(arr_skill_phrases):\n            phrase = Phrase(text=annotation[\"text\"])\n\n            return [phrase]\n\n        # order phrases\n        arr_skill_phrases.sort(key=lambda item: item.start)\n\n        # find non skill phrases\n        arr_non_skill_phrases = []\n\n        # handle case of first skill phrase\n        start = 0\n        end = arr_skill_phrases[0].start - 1\n\n        non_skill_phrase = Phrase(\" \".join(list_words[start:end + 1]))\n        non_skill_phrase.start = start\n        non_skill_phrase.end = end\n\n        arr_non_skill_phrases.append(non_skill_phrase)\n\n        # between skills phrases\n        for i in range(len(arr_skill_phrases) - 1):\n            start = arr_skill_phrases[i].end + 1\n            end = arr_skill_phrases[i + 1].start - 1\n\n            non_skill_phrase = Phrase(\" \".join(list_words[start:end + 1]))\n            non_skill_phrase.start = start\n            non_skill_phrase.end = end\n\n            arr_non_skill_phrases.append(non_skill_phrase)\n\n        # handle case of last skill phrase\n        # handle case of first skill phrase\n        start = arr_skill_phrases[-1].end + 1\n        end = len(list_words) - 1\n\n        non_skill_phrase = Phrase(\" \".join(list_words[start:end + 1]))\n        non_skill_phrase.start = start\n        non_skill_phrase.end = end\n\n        arr_non_skill_phrases.append(non_skill_phrase)\n\n        # merge arr non skill and skill then sort\n        arr_phrases = arr_skill_phrases + arr_non_skill_phrases\n        arr_phrases.sort(key=lambda item: item.start)\n\n        return arr_phrases\n",
      "name_column": 6,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py",
        "skillNer\\skill_extractor_class.py/SkillExtractor/describe",
        "skillNer\\visualizer\\html_elements.py",
        "skillNer\\visualizer\\html_elements.py/render_phrase"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [
        "**__init__**: The function of __init__ is to initialize the Phrase object with the provided text and set default values for its attributes.\n\n**parameters**:\n- self: Represents the instance of the class.\n- text: A string that represents the raw text of the phrase.\n\n**Code Description**:\nIn this function, the raw text of the phrase is saved in the `raw_text` attribute. The `start` and `end` attributes are initialized to None, representing the position of the phrase in the original text. The `is_skill` attribute is set to False by default, indicating whether the phrase is a skill or not. Additionally, the function initializes the following attributes with default values:\n- skill_id: An empty string to store the skill ID.\n- skill_name: An empty string to store the name of the skill.\n- skill_type: A string to specify the type of skill (certification, hard skill, or soft skill).\n- score: Initialized to None to store the score related to the skill.\n- type_matching: A string to indicate the type of matching (full match, n_gram).\n\n**Note**: Ensure to provide the text parameter with the raw text of the phrase when initializing a Phrase object. The other attributes will be initialized with default values unless explicitly set later in the code."
      ],
      "code_start_line": 10,
      "code_end_line": 30,
      "params": [
        "self",
        "text"
      ],
      "have_return": false,
      "code_content": "    def __init__(\n        self,\n        text: str\n    ) -> None:\n\n        # save raw text\n        self.raw_text = text\n\n        # position of phrase in original text\n        self.start: int = None\n        self.end: int = None\n\n        # type of phrase: skill or not\n        self.is_skill: bool = False\n\n        # meta data\n        self.skill_id: str = \"\"\n        self.skill_name: str = \"\"\n        self.skill_type: str = \"\"  # certification, hard skill or soft skill\n        self.score: float = None\n        self.type_matching: str = \"\"  # full match, n_gram\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "get_meta_data",
      "md_content": [
        "**get_meta_data**: The function of get_meta_data is to return a dictionary containing the skill name, matching type, and score of a Phrase object.\n\n**parameters**: \n- self: Represents the instance of the class.\n\n**Code Description**: The get_meta_data function retrieves specific attributes of a Phrase object such as the skill name, matching type, and score. These attributes are then stored in a dictionary format and returned by the function.\n\nIn the calling situation, the get_meta_data function is utilized within the render_phrase function in the html_elements.py file. In this context, the get_meta_data function is used to populate the meta data component of a skill phrase element. The meta data component displays the skill name, matching type, and score of the skill phrase.\n\n**Note**: It is important to ensure that the Phrase object has valid values for skill_name, type_matching, and score attributes before calling the get_meta_data function to avoid any potential errors.\n\n**Output Example**: \n{\n    \"skill name\": \"Python Programming\",\n    \"matching type\": \"Exact Match\",\n    \"score\": 0.85\n}"
      ],
      "code_start_line": 32,
      "code_end_line": 38,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_meta_data(self) -> dict:\n\n        return {\n            \"skill name\": self.skill_name,\n            \"matching type\": self.type_matching,\n            \"score\": self.score\n        }\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\visualizer\\html_elements.py/render_phrase"
      ],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "split_text_to_phare",
      "md_content": [
        "**split_text_to_phare**: The function of split_text_to_phare is to distinguish skill and non-skill phrases within a given text based on annotations and a skill database.\n\n**parameters**:\n- annotation: The output of the ``SkillExtractor.annotate`` function, containing information about the text and annotated skills.\n- SKILL_DB: A dictionary representing the database of skills.\n\n**Code Description**:\nThe `split_text_to_phare` function processes the annotation and skill database to identify skill phrases within the text. It creates Phrase objects for each skill phrase, populates them with relevant information, and organizes them along with non-skill phrases based on word indices. The function then merges and sorts the skill and non-skill phrases before returning them as a list.\n\nIn the calling object `describe` within `skill_extractor_class.py`, the `split_text_to_phare` function is utilized to generate phrases for display based on the annotations obtained from the `annotate` function. These phrases are further rendered using HTML, CSS, and JavaScript to showcase the annotated skills in a visually appealing manner.\n\n**Note**: Ensure that the `annotation` parameter contains the necessary keys such as \"text\" and \"results\" for the function to work correctly. Additionally, the `SKILL_DB` should provide the required skill information for proper identification and categorization.\n\n**Output Example**:\n```python\n[\n    Phrase(text='Non-skill phrase 1', start=0, end=4),\n    Phrase(text='Skill phrase 1', start=5, end=7, is_skill=True, type_matching='type', skill_id='123', skill_name='Skill A', skill_type='Type A', score=0.9),\n    Phrase(text='Non-skill phrase 2', start=8, end=10),\n    ...\n]\n```"
      ],
      "code_start_line": 41,
      "code_end_line": 136,
      "params": [
        "annotation",
        "SKILL_DB"
      ],
      "have_return": true,
      "code_content": "    def split_text_to_phare(\n        annotation: dict,  # result of skill extractor\n        SKILL_DB: dict\n    ) -> List:\n        \"\"\"Main function to distinguish skill and non skill phrases\n\n        Parameters\n        ----------\n        annotation : dict\n            The output of the ``SkillExtractor.annotate``\n        SKILL_DB: dict\n            Data base of skills\n\n        Returns\n        -------\n        List\n            returns a list of phrases\n        \"\"\"\n        # params\n        list_words = annotation[\"text\"].split(\" \")\n\n        # find skill phrases\n        arr_skill_phrases = []\n\n        for type_matching, arr_skills in annotation[\"results\"].items():\n            for skill in arr_skills:\n                # create a phrase object\n                phrase = Phrase(text=skill[\"doc_node_value\"])\n                phrase.is_skill = True\n\n                # index word start and end\n                start = skill[\"doc_node_id\"][0]\n                end = skill[\"doc_node_id\"][-1]\n\n                phrase.start = start\n                phrase.end = end\n\n                # meta data\n                phrase.type_matching = type_matching\n                phrase.skill_id = skill[\"skill_id\"]\n                phrase.skill_name = SKILL_DB[skill[\"skill_id\"]][\"skill_name\"]\n                phrase.skill_type = SKILL_DB[skill[\"skill_id\"]][\"skill_type\"]\n                phrase.score = skill[\"score\"]\n\n                # append phrase\n                arr_skill_phrases.append(phrase)\n\n        # handle case where no skill was annotated in text\n        if not len(arr_skill_phrases):\n            phrase = Phrase(text=annotation[\"text\"])\n\n            return [phrase]\n\n        # order phrases\n        arr_skill_phrases.sort(key=lambda item: item.start)\n\n        # find non skill phrases\n        arr_non_skill_phrases = []\n\n        # handle case of first skill phrase\n        start = 0\n        end = arr_skill_phrases[0].start - 1\n\n        non_skill_phrase = Phrase(\" \".join(list_words[start:end + 1]))\n        non_skill_phrase.start = start\n        non_skill_phrase.end = end\n\n        arr_non_skill_phrases.append(non_skill_phrase)\n\n        # between skills phrases\n        for i in range(len(arr_skill_phrases) - 1):\n            start = arr_skill_phrases[i].end + 1\n            end = arr_skill_phrases[i + 1].start - 1\n\n            non_skill_phrase = Phrase(\" \".join(list_words[start:end + 1]))\n            non_skill_phrase.start = start\n            non_skill_phrase.end = end\n\n            arr_non_skill_phrases.append(non_skill_phrase)\n\n        # handle case of last skill phrase\n        # handle case of first skill phrase\n        start = arr_skill_phrases[-1].end + 1\n        end = len(list_words) - 1\n\n        non_skill_phrase = Phrase(\" \".join(list_words[start:end + 1]))\n        non_skill_phrase.start = start\n        non_skill_phrase.end = end\n\n        arr_non_skill_phrases.append(non_skill_phrase)\n\n        # merge arr non skill and skill then sort\n        arr_phrases = arr_skill_phrases + arr_non_skill_phrases\n        arr_phrases.sort(key=lambda item: item.start)\n\n        return arr_phrases\n",
      "name_column": 8,
      "item_status": "doc_up_to_date",
      "who_reference_me": [
        "skillNer\\skill_extractor_class.py/SkillExtractor/describe"
      ],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "skills_processor\\create_surf_db.py": [
    {
      "type": "FunctionDef",
      "name": "extract_sub_forms",
      "md_content": [
        "**extract_sub_forms**: The function of extract_sub_forms is to extract sub-forms from a given skill name.\n\n**parameters**:\n- skill_name: A string representing the skill name from which sub-forms need to be extracted.\n\n**Code Description**:\nThe extract_sub_forms function takes a skill_name as input and uses the re.finditer method to find all occurrences of a specific pattern (rx) within the skill_name. It then returns a list of all the matched sub-forms.\n\n**Note**:\n- Make sure to define the pattern (rx) before calling the extract_sub_forms function.\n- Ensure that the skill_name provided is a string.\n\n**Output Example**:\nIf the skill_name is \"Python programming language\", and the pattern (rx) is set to r'\\w+', the function may return ['Python', 'programming', 'language']."
      ],
      "code_start_line": 100,
      "code_end_line": 101,
      "params": [
        "skill_name"
      ],
      "have_return": true,
      "code_content": "def extract_sub_forms(skill_name):\n    return [x.group() for x in re.finditer(rx, skill_name)]\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    },
    {
      "type": "FunctionDef",
      "name": "remove_btwn_par",
      "md_content": [
        "**remove_btwn_par**: The function of remove_btwn_par is to remove all text between parentheses and square brackets in a given string.\n\n**parameters**:\n- str_: A string containing text that may include content within parentheses and square brackets.\n\n**Code Description**:\nThe remove_btwn_par function utilizes the re.sub method from the Python re module to substitute any text enclosed within parentheses and square brackets with an empty string in the input string. The regular expression \"[\\(\\[].*?[\\)\\]]\" is used to match and remove the content between the opening and closing parentheses or square brackets, including the brackets themselves.\n\n**Note**:\nIt is important to note that this function will remove all content between the first occurrence of an opening parenthesis or square bracket and the corresponding closing parenthesis or square bracket. If there are nested parentheses or square brackets, only the content within the outermost pair will be removed.\n\n**Output Example**:\nInput: \"Hello (world) [123]!\"\nOutput: \"Hello !\""
      ],
      "code_start_line": 104,
      "code_end_line": 105,
      "params": [
        "str_"
      ],
      "have_return": true,
      "code_content": "def remove_btwn_par(str_):\n    return re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str_)\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "skills_processor\\create_token_dist.py": [
    {
      "type": "FunctionDef",
      "name": "get_dist_new",
      "md_content": [
        "**get_dist_new**: The function of get_dist_new is to count the frequency of each word in a given array and return a dictionary with the word as the key and its frequency as the value.\n\n**parameters**:\n- array: A list of strings containing words.\n\n**Code Description**:\nThe function first initializes an empty list called \"words\". It then iterates through each element in the input array, splits the string by space, and appends each word to the \"words\" list. After that, it assigns the \"words\" list to variable \"a\". The function then uses the collections.Counter() method to count the frequency of each word in the list and converts it into a dictionary format. Finally, it returns the dictionary containing the word frequencies.\n\n**Note**:\n- Make sure to pass a list of strings as the input array parameter to get accurate word frequency counts.\n- Ensure that the input array is not empty to avoid any errors.\n\n**Output Example**:\n{'word1': 2, 'word2': 1, 'word3': 3, ...}"
      ],
      "code_start_line": 12,
      "code_end_line": 23,
      "params": [
        "array"
      ],
      "have_return": true,
      "code_content": "def get_dist_new(array):\n    words = []\n    for val in array:\n        vals = val.split(' ')\n        for v in vals:\n\n            words.append(v)\n\n    a = words\n    counter = collections.Counter(a)\n    counter = dict(counter)\n    return counter\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ],
  "skills_processor\\fetch_raw_data.py": [
    {
      "type": "FunctionDef",
      "name": "fetch_skills_list",
      "md_content": [
        "**fetch_skills_list**: The function of fetch_skills_list is to retrieve a list of skills from a specific endpoint and return the data in a pandas DataFrame format.\n\n**parameters**:\n- No parameters are passed to this function explicitly, but it relies on the access_token variable to construct the authorization header.\n\n**Code Description**:\nThe fetch_skills_list function first defines the endpoint for retrieving all skills. It then constructs an authorization header using the access_token variable. A GET request is made to the endpoint with the authorization header, and the response is stored. The function extracts the 'data' key from the JSON response and returns it.\n\n**Note**:\n- Ensure that the access_token variable is defined and accessible within the scope of this function to construct the authorization header successfully.\n- Make sure the response from the endpoint contains the expected JSON structure with a 'data' key to avoid any errors during data extraction.\n\n**Output Example**:\n```python\n[\n    {\n        \"skill_id\": 1,\n        \"skill_name\": \"Python Programming\",\n        \"category\": \"Programming\"\n    },\n    {\n        \"skill_id\": 2,\n        \"skill_name\": \"Data Analysis\",\n        \"category\": \"Data Science\"\n    },\n    ...\n]\n```"
      ],
      "code_start_line": 25,
      "code_end_line": 37,
      "params": [],
      "have_return": true,
      "code_content": "def fetch_skills_list() -> pd.DataFrame:\n\n    # List of all skills endpoint\n    all_skills_endpoint = \"https://emsiservices.com/skills/versions/latest/skills\"\n    # Auth string including access token from above\n    auth = f\"Authorization: Bearer {access_token}\"\n    headers = {'authorization': auth}  # headers\n    response = requests.request(\n        \"GET\", all_skills_endpoint, headers=headers)  # response\n    response = response.json()['data']  # the data\n\n    # all_skills_df = pd.DataFrame(json_normalize(response)); # Where response is a JSON object drilled down to the level of 'data' key\n    return response\n",
      "name_column": 4,
      "item_status": "doc_up_to_date",
      "who_reference_me": [],
      "reference_who": [],
      "special_reference_type": []
    }
  ]
}